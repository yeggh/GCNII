{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b54ac78b-85cb-402c-878f-dbbafbd97116",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, List, Optional, Tuple\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn. functional as F\n",
    "import torch_geometric.transforms as T\n",
    "from torch import Tensor\n",
    "from torch.optim import Optimizer\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.datasets import Planetoid, WebKB\n",
    "from torch_geometric.utils import accuracy\n",
    "from typing_extensions import Literal, TypedDict\n",
    "from collections import defaultdict\n",
    "from typing import List, Optional, Tuple, Union\n",
    "import scipy.sparse\n",
    "from torch import Tensor\n",
    "from scipy.sparse import coo_matrix, eye, diags, csr_matrix\n",
    "import torch.nn as nn\n",
    "from torch.nn import init\n",
    "from torch.nn import Parameter\n",
    "import torch.nn.functional as F\n",
    "from functools import cached_property\n",
    "import scipy.sparse as sp\n",
    "from torch_geometric.transforms import RandomNodeSplit\n",
    "from torch_geometric.datasets.wikipedia_network import WikipediaNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ef577f85-b6dd-4a1d-a337-0c9ad90e0922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load the dataset \n",
    "\n",
    "def get_dataset(path, name, split, transform):\n",
    "    if name in ('Cora', 'CiteSeer', 'PubMed'):\n",
    "        dataset = Planetoid(path, name=name, split=split, transform=transform)\n",
    "    elif name in ('Cornell', 'Texas', 'Wisconsin'):\n",
    "        dataset = WebKB(path, name=name, transform=transform)\n",
    "    elif name == 'chameleon':\n",
    "        dataset = WikipediaNetwork(path, name=name, geom_gcn_preprocess= True, transform=transform)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown dataset name: {name}\")\n",
    "\n",
    "    num_nodes = dataset.data.num_nodes\n",
    "    num_edges = dataset.data.num_edges // 2\n",
    "    print(f\"Dataset: {dataset.name}\")\n",
    "    print(f\"Num. nodes: {num_nodes}\")\n",
    "    print(f\"Num. edges: {num_edges}\")\n",
    "    print(f\"Num. node features: {dataset.num_node_features}\")\n",
    "    print(f\"Num. classes: {dataset.num_classes}\")\n",
    "    print(f\"Dataset len.: {dataset.len()}\")\n",
    "    # print(f\"Sum of row values with normalization: {dataset[0].x.sum(dim=-1)}\")\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6d76a21-f46b-4ad7-8c07-19aafca3ad7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to randomly split nodes of each class into 60%, 20%, and 20% for training, validation and testing based on the paper\n",
    "def split_dataset(dataset, train_ratio=0.6, val_ratio=0.2, test_ratio=0.2):\n",
    "    num_nodes = dataset[0].num_nodes\n",
    "    split = RandomNodeSplit(split=\"random\", num_splits=1, num_train_per_class=int((num_nodes) / 7 * train_ratio), num_val=int(num_nodes * val_ratio), num_test=int(num_nodes * test_ratio))\n",
    "    splitted_dataset = split(dataset[0])\n",
    "    train_len = splitted_dataset.train_mask.sum()\n",
    "    val_len = splitted_dataset.val_mask.sum()\n",
    "    test_len = splitted_dataset.test_mask.sum()\n",
    "    other_len = num_nodes - train_len-val_len - test_len\n",
    "    print(f\"Num. train={train_len}, val={val_len}, test={test_len}, other={other_len}\")\n",
    "    return splitted_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aed475f8-9db7-4a7f-a08c-c3ba40e05b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create an adjacency matrix from edge index and edge attributes\n",
    "\n",
    "def adjacency_matrix(edge_index, edge_attr=None, num_nodes=None):\n",
    "    # Calculate the number of nodes if not provided\n",
    "    if num_nodes is None:\n",
    "        num_nodes = int(edge_index.max()) + 1\n",
    "    # If edge_attr does not exist, assign weight=1 to all edges\n",
    "    if edge_attr is None:\n",
    "        edge_attr = torch.ones(edge_index.shape[1], dtype=torch.float)\n",
    "    # Define the sparse adjacency matrix\n",
    "    adj_matrix_sparse = torch.sparse_coo_tensor(edge_index, edge_attr, (num_nodes, num_nodes))\n",
    "    # Convert to a dense matrix\n",
    "    adj_matrix = adj_matrix_sparse.to_dense()\n",
    "    return (adj_matrix + adj_matrix.T) / 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72c79567-fb40-4655-bcb3-8ec14f4c4c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the normalize adjancy matrix based on the paper P ̃ = (D ̃^ −1/2)A ̃(D ̃^ −1/2 )\n",
    "\n",
    "def normalize_adjacency_matrix(A):\n",
    "    # Ensure A is a sparse matrix\n",
    "    if not isinstance(A, csr_matrix):\n",
    "        A = csr_matrix(A)\n",
    "    A = A + sp.eye(A.shape[0])\n",
    "    degrees = np.array(A.sum(axis=1)).flatten()\n",
    "    degrees[degrees == 0] = 1  # Replace 0s with 1s\n",
    "    D_inv_sqrt = diags(1.0 / np.sqrt(degrees))\n",
    "    normalized_A = D_inv_sqrt @ A @ D_inv_sqrt\n",
    "    return normalized_A\n",
    "\n",
    "\n",
    "# Convert the matrix to Tensor\n",
    "def sparse_matrix_to_torch_sparse_tensor(sparse_matrix):\n",
    "    sparse_matrix = sparse_matrix.tocoo()\n",
    "    indices = torch.LongTensor(np.vstack((sparse_matrix.row, sparse_matrix.col)))\n",
    "    values = torch.FloatTensor(sparse_matrix.data)\n",
    "    shape = torch.Size(sparse_matrix.shape)\n",
    "    return torch.sparse.FloatTensor(indices, values, shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e97b9162-fdc8-4fd5-bdb8-bd04a77da246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition the CGNII model\n",
    "\n",
    "class GCNLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, in_features, out_features):\n",
    "\n",
    "        super(GCNLayer, self).__init__() \n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = torch.nn.Parameter(init.kaiming_uniform_(torch.empty(self.in_features, self.out_features), mode='fan_in', nonlinearity='relu'))\n",
    "\n",
    "    def forward(self, input, adj , h_0 , lamda, alpha, l):\n",
    "\n",
    "        h_l = torch.spmm(adj, input)\n",
    "        features = (1 - alpha) * h_l + alpha * h_0\n",
    "        n = self.weight.shape[0]\n",
    "        I_n = torch.eye(n) \n",
    "        beta = np.log((lamda / l) + 1)\n",
    "        term1 = (1 - beta) * I_n\n",
    "        term2 = beta * self.weight\n",
    "        weights = term1 + term2\n",
    "        output = torch.mm(features, weights)\n",
    "        return output\n",
    "\n",
    "class GCNII(nn.Module):\n",
    "    def __init__(self, nfeat, nlayers, nhidden, nclass, dropout, lamda, alpha):\n",
    "        super(GCNII, self).__init__()\n",
    "        self.graph_convs = nn.ModuleList()  \n",
    "        for i in range(nlayers):\n",
    "            conv_layer = GCNLayer(nhidden, nhidden)\n",
    "            self.graph_convs.append(conv_layer)\n",
    "\n",
    "        self.pre_fc = nn.Linear(nfeat, nhidden)\n",
    "        self.post_fc = nn.Linear(nhidden, nclass)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = dropout\n",
    "        self.lamda = lamda\n",
    "        self.alpha = alpha\n",
    "\n",
    "    # Forward pass accepts edge_index and edge_attr\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        # Construct the adjacency matrix from edge_index and edge_attr\n",
    "        adj = adjacency_matrix(edge_index, edge_attr)\n",
    "        adj = normalize_adjacency_matrix(adj)\n",
    "        adj = sparse_matrix_to_torch_sparse_tensor(adj)\n",
    "\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        h_0 = self.relu(self.pre_fc(x))\n",
    "        h = h_0\n",
    "        for i, con in enumerate(self.graph_convs):\n",
    "            h = F.dropout(h, self.dropout, training=self.training)\n",
    "            h = self.relu(con(h, adj, h_0, self.lamda, self.alpha, i + 1))\n",
    "        h = F.dropout(h, self.dropout, training=self.training)\n",
    "        h = self.post_fc(h)\n",
    "        return F.log_softmax(h, dim=1)\n",
    "    \n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    pass\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3d4ef943-7334-410f-8112-0ddfad3611d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "LossFunction = Callable[[Tensor, Tensor], Tensor]\n",
    "Stage = Literal[\"train\", \"val\", \"test\"]\n",
    "\n",
    "def train_step(\n",
    "    model: torch.nn.Module, data: Data, optimizer: torch.optim.Optimizer, loss_function: LossFunction,split:int =1\n",
    ") -> Tuple[float, float]:\n",
    "    # Set model to training mode\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    training_mask = data.train_mask[:, split]\n",
    "    logits = model(data.x, data.edge_index, data.edge_attr)[training_mask]\n",
    "    predictions = torch.argmax(logits, dim=1)\n",
    "    labels = data.y[training_mask]\n",
    "    loss = loss_function(logits, labels)\n",
    "    acc = accuracy(predictions, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()  \n",
    "    return loss.item(), acc\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_step(\n",
    "    model: torch.nn.Module, data: Data, loss_function: LossFunction, stage: Stage,split:int =1\n",
    ") -> Tuple[float, float]:\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "    stage_mask = getattr(data, f\"{stage}_mask\")\n",
    "    stage_mask = stage_mask[:, split]\n",
    "    logits = model(data.x, data.edge_index, data.edge_attr)[stage_mask]\n",
    "    predictions = torch.argmax(logits, dim=1)  \n",
    "    labels = data.y[stage_mask]\n",
    "    loss = loss_function(logits, labels)\n",
    "    acc = accuracy(predictions, labels)  \n",
    "    return loss.item(), acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "56a8b538-13bf-424e-b04f-453720932cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HistoryDict(TypedDict):\n",
    "    loss: List[float]\n",
    "    acc: List[float]\n",
    "    val_loss: List[float]\n",
    "    val_acc: List[float]\n",
    "\n",
    "def train(\n",
    "    model: torch.nn.Module,\n",
    "    data: Data,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    loss_function: LossFunction = torch.nn.CrossEntropyLoss(),\n",
    "    max_epochs: int = 1500,\n",
    "    early_stopping: int = 100,\n",
    "    print_interval: int = 100,\n",
    "    verbose: bool = True,\n",
    "    split: int = 1\n",
    ") -> HistoryDict:\n",
    "    history = {\"loss\": [], \"val_loss\": [], \"acc\": [], \"val_acc\": []}\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(max_epochs):\n",
    "        # Perform a training step\n",
    "        loss, acc = train_step(model, data, optimizer, loss_function,split)\n",
    "        # Perform an evaluation step\n",
    "        val_loss, val_acc = evaluate_step(model, data, loss_function, \"val\",split)\n",
    "\n",
    "        # Update history\n",
    "        history[\"loss\"].append(loss)\n",
    "        history[\"acc\"].append(acc)\n",
    "        history[\"val_loss\"].append(val_loss)\n",
    "        history[\"val_acc\"].append(val_acc)\n",
    "\n",
    "        if epoch > early_stopping and val_loss > np.mean(history[\"val_loss\"][-(early_stopping + 1):-1]):\n",
    "            if verbose:\n",
    "                print(\"\\nEarly stopping...\")\n",
    "            break\n",
    "\n",
    "        # Print training progress\n",
    "        if verbose and epoch % print_interval == 0:\n",
    "            print(f\"\\nEpoch: {epoch}\\n------\")\n",
    "            print(f\"Train loss: {loss:.4f} | Train acc: {acc:.4f}\")\n",
    "            print(f\"Val loss: {val_loss:.4f} | Val acc: {val_acc:.4f}\")\n",
    "\n",
    "    # Perform a final evaluation step\n",
    "    test_loss, test_acc = evaluate_step(model, data, loss_function, \"test\")\n",
    "    # Print final results\n",
    "    # if verbose:\n",
    "    #     print(f\"\\nEpoch: {epoch}\\n------\")\n",
    "    #     print(f\" Train loss: {loss:.4f} | Train acc: {acc:.4f}\")\n",
    "    #     print(f\" Val loss: {val_loss:.4f} | Val acc: {val_acc:.4f}\")\n",
    "    #     print(f\" Test loss: {test_loss:.4f} | Test acc: {test_acc:.4f}\")\n",
    "    return history, test_loss, test_acc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "32f22273-baff-49b0-aa6c-525f245ade37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: cornell\n",
      "Num. nodes: 183\n",
      "Num. edges: 149\n",
      "Num. node features: 1703\n",
      "Num. classes: 5\n",
      "Dataset len.: 1\n",
      "\n",
      "Epoch: 0\n",
      "------\n",
      "Train loss: 1.6172 | Train acc: 0.1839\n",
      "Val loss: 1.6039 | Val acc: 0.1864\n",
      "\n",
      "Epoch: 100\n",
      "------\n",
      "Train loss: 0.9675 | Train acc: 0.7126\n",
      "Val loss: 1.0223 | Val acc: 0.6949\n",
      "\n",
      "Epoch: 200\n",
      "------\n",
      "Train loss: 0.6943 | Train acc: 0.7586\n",
      "Val loss: 0.8906 | Val acc: 0.6780\n",
      "\n",
      "Early stopping...\n",
      "\n",
      "Epoch: 0\n",
      "------\n",
      "Train loss: 1.6252 | Train acc: 0.2299\n",
      "Val loss: 1.6193 | Val acc: 0.2203\n",
      "\n",
      "Epoch: 100\n",
      "------\n",
      "Train loss: 0.7917 | Train acc: 0.7586\n",
      "Val loss: 0.9861 | Val acc: 0.7119\n",
      "\n",
      "Epoch: 200\n",
      "------\n",
      "Train loss: 0.5220 | Train acc: 0.8391\n",
      "Val loss: 0.8423 | Val acc: 0.7119\n",
      "\n",
      "Early stopping...\n",
      "\n",
      "Epoch: 0\n",
      "------\n",
      "Train loss: 1.6469 | Train acc: 0.1954\n",
      "Val loss: 1.6275 | Val acc: 0.1695\n",
      "\n",
      "Epoch: 100\n",
      "------\n",
      "Train loss: 0.8623 | Train acc: 0.7586\n",
      "Val loss: 1.2637 | Val acc: 0.5763\n",
      "\n",
      "Epoch: 200\n",
      "------\n",
      "Train loss: 0.6122 | Train acc: 0.8506\n",
      "Val loss: 1.1606 | Val acc: 0.5424\n",
      "\n",
      "Early stopping...\n",
      "\n",
      "Epoch: 0\n",
      "------\n",
      "Train loss: 1.6257 | Train acc: 0.0920\n",
      "Val loss: 1.6140 | Val acc: 0.1186\n",
      "\n",
      "Epoch: 100\n",
      "------\n",
      "Train loss: 0.8669 | Train acc: 0.6552\n",
      "Val loss: 1.2606 | Val acc: 0.5424\n",
      "\n",
      "Epoch: 200\n",
      "------\n",
      "Train loss: 0.5890 | Train acc: 0.8276\n",
      "Val loss: 1.0700 | Val acc: 0.6441\n",
      "\n",
      "Epoch: 300\n",
      "------\n",
      "Train loss: 0.4828 | Train acc: 0.8506\n",
      "Val loss: 1.0144 | Val acc: 0.6441\n",
      "\n",
      "Early stopping...\n",
      "\n",
      "Epoch: 0\n",
      "------\n",
      "Train loss: 1.5873 | Train acc: 0.4253\n",
      "Val loss: 1.5749 | Val acc: 0.4746\n",
      "\n",
      "Epoch: 100\n",
      "------\n",
      "Train loss: 0.7439 | Train acc: 0.7931\n",
      "Val loss: 1.2569 | Val acc: 0.5593\n",
      "\n",
      "Early stopping...\n",
      "\n",
      "Epoch: 0\n",
      "------\n",
      "Train loss: 1.6119 | Train acc: 0.1034\n",
      "Val loss: 1.6007 | Val acc: 0.1186\n",
      "\n",
      "Epoch: 100\n",
      "------\n",
      "Train loss: 0.7938 | Train acc: 0.7356\n",
      "Val loss: 1.2499 | Val acc: 0.5932\n",
      "\n",
      "Epoch: 200\n",
      "------\n",
      "Train loss: 0.6198 | Train acc: 0.8161\n",
      "Val loss: 1.1805 | Val acc: 0.5763\n",
      "\n",
      "Early stopping...\n",
      "\n",
      "Epoch: 0\n",
      "------\n",
      "Train loss: 1.6200 | Train acc: 0.1609\n",
      "Val loss: 1.6000 | Val acc: 0.2373\n",
      "\n",
      "Epoch: 100\n",
      "------\n",
      "Train loss: 0.9092 | Train acc: 0.6322\n",
      "Val loss: 1.3244 | Val acc: 0.5085\n",
      "\n",
      "Early stopping...\n",
      "\n",
      "Epoch: 0\n",
      "------\n",
      "Train loss: 1.6392 | Train acc: 0.0920\n",
      "Val loss: 1.6326 | Val acc: 0.0847\n",
      "\n",
      "Epoch: 100\n",
      "------\n",
      "Train loss: 0.8694 | Train acc: 0.7241\n",
      "Val loss: 0.9936 | Val acc: 0.6441\n",
      "\n",
      "Epoch: 200\n",
      "------\n",
      "Train loss: 0.5863 | Train acc: 0.8736\n",
      "Val loss: 0.8246 | Val acc: 0.6780\n",
      "\n",
      "Early stopping...\n",
      "\n",
      "Epoch: 0\n",
      "------\n",
      "Train loss: 1.5889 | Train acc: 0.4138\n",
      "Val loss: 1.5427 | Val acc: 0.5593\n",
      "\n",
      "Epoch: 100\n",
      "------\n",
      "Train loss: 0.7849 | Train acc: 0.7701\n",
      "Val loss: 1.0327 | Val acc: 0.6949\n",
      "\n",
      "Early stopping...\n",
      "\n",
      "Epoch: 0\n",
      "------\n",
      "Train loss: 1.6178 | Train acc: 0.1379\n",
      "Val loss: 1.6053 | Val acc: 0.0339\n",
      "\n",
      "Epoch: 100\n",
      "------\n",
      "Train loss: 0.8326 | Train acc: 0.6667\n",
      "Val loss: 0.9882 | Val acc: 0.7458\n",
      "\n",
      "Early stopping...\n",
      "Mean Test Accuracy: 0.7243\n",
      "Standard Deviation of Test Accuracies: 0.0911\n"
     ]
    }
   ],
   "source": [
    "# Load the Cornell dataset for full-supervised task: \n",
    "dataset = get_dataset(path = \"/tmp/Cornell\", name=\"Cornell\",split = \"full\", transform=T.NormalizeFeatures())\n",
    "data = dataset[0]\n",
    "\n",
    "\n",
    "# Training configuration of GCNII for full-supervised task based on Table 7 of the paper\n",
    "SEED = 42\n",
    "NLAYERS = 16\n",
    "ALPHA = 0.5\n",
    "LEARNING_RATE = 0.01\n",
    "NHIDDEN = 64\n",
    "LAMBDA = 1\n",
    "DROPOUT = 0.5\n",
    "MAX_EPOCHS = 1500\n",
    "WEIGHT_DECAY =  0.001\n",
    "EARLY_STOPPING = 100\n",
    "\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "test_accuracies = []\n",
    "\n",
    "# Run the training and evaluation process 10 times with random splits\n",
    "for i in range(10):\n",
    "    # Get a random split of the dataset\n",
    "    Split = i\n",
    "    \n",
    "    \n",
    "    # Initialize the model\n",
    "    model = GCNII(nfeat=data.num_node_features,\n",
    "                  nlayers=NLAYERS,\n",
    "                  nhidden=NHIDDEN,\n",
    "                  nclass=dataset.num_classes,\n",
    "                  dropout=DROPOUT,\n",
    "                  lamda=LAMBDA,\n",
    "                  alpha=ALPHA)\n",
    "    \n",
    "    # Define the optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "    # Train and evaluate the model on the current split\n",
    "    history, test_loss, test_acc  = train(model, data, optimizer, max_epochs=MAX_EPOCHS, early_stopping=EARLY_STOPPING,split = Split)\n",
    "    \n",
    "    # Store the test accuracy result\n",
    "    test_accuracies.append(test_acc)\n",
    "\n",
    "# Calculate the mean and standard deviation of test accuracies\n",
    "mean_accuracy = np.mean(test_accuracies)\n",
    "std_accuracy = np.std(test_accuracies)\n",
    "\n",
    "print(f'Mean Test Accuracy: {mean_accuracy:.4f}')\n",
    "print(f'Standard Deviation of Test Accuracies: {std_accuracy:.4f}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3d5ad859-764a-4424-8e03-20961c037055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: texas\n",
      "Num. nodes: 183\n",
      "Num. edges: 162\n",
      "Num. node features: 1703\n",
      "Num. classes: 5\n",
      "Dataset len.: 1\n",
      "\n",
      "Epoch: 0\n",
      "------\n",
      "Train loss: 1.6093 | Train acc: 0.0805\n",
      "Val loss: 1.5846 | Val acc: 0.1186\n",
      "\n",
      "Epoch: 100\n",
      "------\n",
      "Train loss: 0.2669 | Train acc: 0.9080\n",
      "Val loss: 0.8776 | Val acc: 0.6610\n",
      "\n",
      "Early stopping...\n",
      "\n",
      "Epoch: 0\n",
      "------\n",
      "Train loss: 1.6237 | Train acc: 0.0000\n",
      "Val loss: 1.6086 | Val acc: 0.0169\n",
      "\n",
      "Epoch: 100\n",
      "------\n",
      "Train loss: 0.3348 | Train acc: 0.8621\n",
      "Val loss: 0.8517 | Val acc: 0.6949\n",
      "\n",
      "Early stopping...\n",
      "\n",
      "Epoch: 0\n",
      "------\n",
      "Train loss: 1.6048 | Train acc: 0.0920\n",
      "Val loss: 1.5935 | Val acc: 0.5424\n",
      "\n",
      "Epoch: 100\n",
      "------\n",
      "Train loss: 0.3688 | Train acc: 0.9080\n",
      "Val loss: 0.7344 | Val acc: 0.7797\n",
      "\n",
      "Early stopping...\n",
      "\n",
      "Epoch: 0\n",
      "------\n",
      "Train loss: 1.5998 | Train acc: 0.1264\n",
      "Val loss: 1.5781 | Val acc: 0.5254\n",
      "\n",
      "Epoch: 100\n",
      "------\n",
      "Train loss: 0.4210 | Train acc: 0.8391\n",
      "Val loss: 0.7811 | Val acc: 0.7966\n",
      "\n",
      "Epoch: 200\n",
      "------\n",
      "Train loss: 0.2282 | Train acc: 0.9425\n",
      "Val loss: 0.6398 | Val acc: 0.7797\n",
      "\n",
      "Early stopping...\n",
      "\n",
      "Epoch: 0\n",
      "------\n",
      "Train loss: 1.6406 | Train acc: 0.1839\n",
      "Val loss: 1.6313 | Val acc: 0.1695\n",
      "\n",
      "Epoch: 100\n",
      "------\n",
      "Train loss: 0.2916 | Train acc: 0.9310\n",
      "Val loss: 0.7354 | Val acc: 0.7627\n",
      "\n",
      "Early stopping...\n",
      "\n",
      "Epoch: 0\n",
      "------\n",
      "Train loss: 1.6019 | Train acc: 0.5057\n",
      "Val loss: 1.5749 | Val acc: 0.5932\n",
      "\n",
      "Epoch: 100\n",
      "------\n",
      "Train loss: 0.3713 | Train acc: 0.9195\n",
      "Val loss: 0.7658 | Val acc: 0.6949\n",
      "\n",
      "Epoch: 200\n",
      "------\n",
      "Train loss: 0.1486 | Train acc: 0.9655\n",
      "Val loss: 0.6820 | Val acc: 0.7797\n",
      "\n",
      "Early stopping...\n",
      "\n",
      "Epoch: 0\n",
      "------\n",
      "Train loss: 1.6446 | Train acc: 0.1724\n",
      "Val loss: 1.6285 | Val acc: 0.1186\n",
      "\n",
      "Epoch: 100\n",
      "------\n",
      "Train loss: 0.3321 | Train acc: 0.9425\n",
      "Val loss: 0.8705 | Val acc: 0.7627\n",
      "\n",
      "Early stopping...\n",
      "\n",
      "Epoch: 0\n",
      "------\n",
      "Train loss: 1.5957 | Train acc: 0.1379\n",
      "Val loss: 1.5694 | Val acc: 0.0508\n",
      "\n",
      "Epoch: 100\n",
      "------\n",
      "Train loss: 0.3590 | Train acc: 0.8736\n",
      "Val loss: 0.8084 | Val acc: 0.7627\n",
      "\n",
      "Early stopping...\n",
      "\n",
      "Epoch: 0\n",
      "------\n",
      "Train loss: 1.6115 | Train acc: 0.0115\n",
      "Val loss: 1.6014 | Val acc: 0.4915\n",
      "\n",
      "Epoch: 100\n",
      "------\n",
      "Train loss: 0.4358 | Train acc: 0.8276\n",
      "Val loss: 0.8566 | Val acc: 0.5932\n",
      "\n",
      "Epoch: 200\n",
      "------\n",
      "Train loss: 0.1475 | Train acc: 0.9885\n",
      "Val loss: 0.6590 | Val acc: 0.7288\n",
      "\n",
      "Early stopping...\n",
      "\n",
      "Epoch: 0\n",
      "------\n",
      "Train loss: 1.5684 | Train acc: 0.5862\n",
      "Val loss: 1.5717 | Val acc: 0.4576\n",
      "\n",
      "Epoch: 100\n",
      "------\n",
      "Train loss: 0.3939 | Train acc: 0.8506\n",
      "Val loss: 1.1667 | Val acc: 0.6102\n",
      "\n",
      "Early stopping...\n",
      "Mean Test Accuracy: 0.8243\n",
      "Standard Deviation of Test Accuracies: 0.0583\n"
     ]
    }
   ],
   "source": [
    "# Load the Texas dataset for full-supervised task: \n",
    "dataset = get_dataset(path = \"/tmp/Texas\", name=\"Texas\",split = \"full\", transform=T.NormalizeFeatures())\n",
    "data = dataset[0]\n",
    "\n",
    "\n",
    "# Training configuration of GCNII for full-supervised task based on Table 7 of the paper\n",
    "SEED = 42\n",
    "NLAYERS = 32\n",
    "ALPHA = 0.5\n",
    "LEARNING_RATE = 0.01\n",
    "NHIDDEN = 64\n",
    "LAMBDA = 1.5\n",
    "DROPOUT = 0.5\n",
    "MAX_EPOCHS = 1500\n",
    "WEIGHT_DECAY =  0.0001\n",
    "EARLY_STOPPING = 100\n",
    "\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "test_accuracies = []\n",
    "\n",
    "# Run the training and evaluation process 10 times with random splits\n",
    "for i in range(10):\n",
    "    # Get a random split of the dataset\n",
    "    Split = i\n",
    "    \n",
    "    \n",
    "    # Initialize the model\n",
    "    model = GCNII(nfeat=data.num_node_features,\n",
    "                  nlayers=NLAYERS,\n",
    "                  nhidden=NHIDDEN,\n",
    "                  nclass=dataset.num_classes,\n",
    "                  dropout=DROPOUT,\n",
    "                  lamda=LAMBDA,\n",
    "                  alpha=ALPHA)\n",
    "    \n",
    "    # Define the optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "    # Train and evaluate the model on the current split\n",
    "    history, test_loss, test_acc  = train(model, data, optimizer, max_epochs=MAX_EPOCHS, early_stopping=EARLY_STOPPING,split = Split)\n",
    "    \n",
    "    # Store the test accuracy result\n",
    "    test_accuracies.append(test_acc)\n",
    "\n",
    "# Calculate the mean and standard deviation of test accuracies\n",
    "mean_accuracy = np.mean(test_accuracies)\n",
    "std_accuracy = np.std(test_accuracies)\n",
    "\n",
    "print(f'Mean Test Accuracy: {mean_accuracy:.4f}')\n",
    "print(f'Standard Deviation of Test Accuracies: {std_accuracy:.4f}')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "600aa2f1-83aa-468e-ace5-430f9dcb2186",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: wisconsin\n",
      "Num. nodes: 251\n",
      "Num. edges: 257\n",
      "Num. node features: 1703\n",
      "Num. classes: 5\n",
      "Dataset len.: 1\n",
      "\n",
      "Epoch: 0\n",
      "------\n",
      "Train loss: 1.6167 | Train acc: 0.0333\n",
      "Val loss: 1.5958 | Val acc: 0.2500\n",
      "\n",
      "Epoch: 100\n",
      "------\n",
      "Train loss: 0.5749 | Train acc: 0.7917\n",
      "Val loss: 0.6921 | Val acc: 0.7250\n",
      "\n",
      "Epoch: 200\n",
      "------\n",
      "Train loss: 0.3910 | Train acc: 0.9083\n",
      "Val loss: 0.6132 | Val acc: 0.7625\n",
      "\n",
      "Early stopping...\n",
      "\n",
      "Epoch: 0\n",
      "------\n",
      "Train loss: 1.5727 | Train acc: 0.4583\n",
      "Val loss: 1.5722 | Val acc: 0.3875\n",
      "\n",
      "Epoch: 100\n",
      "------\n",
      "Train loss: 0.4798 | Train acc: 0.8333\n",
      "Val loss: 1.0169 | Val acc: 0.7250\n",
      "\n",
      "Epoch: 200\n",
      "------\n",
      "Train loss: 0.3258 | Train acc: 0.9250\n",
      "Val loss: 0.9305 | Val acc: 0.7375\n",
      "\n",
      "Early stopping...\n",
      "\n",
      "Epoch: 0\n",
      "------\n",
      "Train loss: 1.6362 | Train acc: 0.0833\n",
      "Val loss: 1.6202 | Val acc: 0.0875\n",
      "\n",
      "Epoch: 100\n",
      "------\n",
      "Train loss: 0.5490 | Train acc: 0.8000\n",
      "Val loss: 0.8532 | Val acc: 0.6625\n",
      "\n",
      "Epoch: 200\n",
      "------\n",
      "Train loss: 0.4338 | Train acc: 0.8417\n",
      "Val loss: 0.7861 | Val acc: 0.7375\n",
      "\n",
      "Early stopping...\n",
      "\n",
      "Epoch: 0\n",
      "------\n",
      "Train loss: 1.6100 | Train acc: 0.0417\n",
      "Val loss: 1.5887 | Val acc: 0.4875\n",
      "\n",
      "Epoch: 100\n",
      "------\n",
      "Train loss: 0.5971 | Train acc: 0.8000\n",
      "Val loss: 0.8163 | Val acc: 0.7500\n",
      "\n",
      "Early stopping...\n",
      "\n",
      "Epoch: 0\n",
      "------\n",
      "Train loss: 1.6072 | Train acc: 0.1750\n",
      "Val loss: 1.5829 | Val acc: 0.5125\n",
      "\n",
      "Epoch: 100\n",
      "------\n",
      "Train loss: 0.5213 | Train acc: 0.8333\n",
      "Val loss: 0.8604 | Val acc: 0.7375\n",
      "\n",
      "Early stopping...\n",
      "\n",
      "Epoch: 0\n",
      "------\n",
      "Train loss: 1.6177 | Train acc: 0.0333\n",
      "Val loss: 1.5993 | Val acc: 0.2625\n",
      "\n",
      "Epoch: 100\n",
      "------\n",
      "Train loss: 0.5337 | Train acc: 0.8417\n",
      "Val loss: 0.7274 | Val acc: 0.7125\n",
      "\n",
      "Epoch: 200\n",
      "------\n",
      "Train loss: 0.3940 | Train acc: 0.8750\n",
      "Val loss: 0.6329 | Val acc: 0.7625\n",
      "\n",
      "Early stopping...\n",
      "\n",
      "Epoch: 0\n",
      "------\n",
      "Train loss: 1.6199 | Train acc: 0.1167\n",
      "Val loss: 1.5899 | Val acc: 0.4000\n",
      "\n",
      "Epoch: 100\n",
      "------\n",
      "Train loss: 0.6205 | Train acc: 0.8000\n",
      "Val loss: 0.7760 | Val acc: 0.7125\n",
      "\n",
      "Early stopping...\n",
      "\n",
      "Epoch: 0\n",
      "------\n",
      "Train loss: 1.5858 | Train acc: 0.1417\n",
      "Val loss: 1.5524 | Val acc: 0.4875\n",
      "\n",
      "Epoch: 100\n",
      "------\n",
      "Train loss: 0.6495 | Train acc: 0.7417\n",
      "Val loss: 0.7017 | Val acc: 0.6875\n",
      "\n",
      "Epoch: 200\n",
      "------\n",
      "Train loss: 0.5073 | Train acc: 0.8250\n",
      "Val loss: 0.6057 | Val acc: 0.7750\n",
      "\n",
      "Early stopping...\n",
      "\n",
      "Epoch: 0\n",
      "------\n",
      "Train loss: 1.6082 | Train acc: 0.1000\n",
      "Val loss: 1.5742 | Val acc: 0.3750\n",
      "\n",
      "Epoch: 100\n",
      "------\n",
      "Train loss: 0.5859 | Train acc: 0.7750\n",
      "Val loss: 0.9381 | Val acc: 0.6875\n",
      "\n",
      "Early stopping...\n",
      "\n",
      "Epoch: 0\n",
      "------\n",
      "Train loss: 1.5817 | Train acc: 0.3250\n",
      "Val loss: 1.5782 | Val acc: 0.4625\n",
      "\n",
      "Epoch: 100\n",
      "------\n",
      "Train loss: 0.5749 | Train acc: 0.7917\n",
      "Val loss: 0.8660 | Val acc: 0.6500\n",
      "\n",
      "Epoch: 200\n",
      "------\n",
      "Train loss: 0.4426 | Train acc: 0.8417\n",
      "Val loss: 0.7806 | Val acc: 0.7000\n",
      "\n",
      "Early stopping...\n",
      "Mean Test Accuracy: 0.8608\n",
      "Standard Deviation of Test Accuracies: 0.0397\n"
     ]
    }
   ],
   "source": [
    "# Load the Wisconsin dataset for full-supervised task: \n",
    "dataset = get_dataset(path = \"/tmp/Wisconsin\", name=\"Wisconsin\",split = \"full\", transform=T.NormalizeFeatures())\n",
    "data = dataset[0]\n",
    "\n",
    "\n",
    "# Training configuration of GCNII for full-supervised task based on Table 7 of the paper\n",
    "SEED = 42\n",
    "NLAYERS = 16\n",
    "ALPHA = 0.5\n",
    "LEARNING_RATE = 0.01\n",
    "NHIDDEN = 64\n",
    "LAMBDA = 1\n",
    "DROPOUT = 0.5\n",
    "MAX_EPOCHS = 1500\n",
    "WEIGHT_DECAY = 0.0005\n",
    "EARLY_STOPPING = 100\n",
    "\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "test_accuracies = []\n",
    "\n",
    "# Run the training and evaluation process 10 times with random splits\n",
    "for i in range(10):\n",
    "    # Get a random split of the dataset\n",
    "    Split = i\n",
    "    \n",
    "    \n",
    "    # Initialize the model\n",
    "    model = GCNII(nfeat=data.num_node_features,\n",
    "                  nlayers=NLAYERS,\n",
    "                  nhidden=NHIDDEN,\n",
    "                  nclass=dataset.num_classes,\n",
    "                  dropout=DROPOUT,\n",
    "                  lamda=LAMBDA,\n",
    "                  alpha=ALPHA)\n",
    "    \n",
    "    # Define the optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "    # Train and evaluate the model on the current split\n",
    "    history, test_loss, test_acc  = train(model, data, optimizer, max_epochs=MAX_EPOCHS, early_stopping=EARLY_STOPPING,split = Split)\n",
    "    \n",
    "    # Store the test accuracy result\n",
    "    test_accuracies.append(test_acc)\n",
    "\n",
    "# Calculate the mean and standard deviation of test accuracies\n",
    "mean_accuracy = np.mean(test_accuracies)\n",
    "std_accuracy = np.std(test_accuracies)\n",
    "\n",
    "print(f'Mean Test Accuracy: {mean_accuracy:.4f}')\n",
    "print(f'Standard Deviation of Test Accuracies: {std_accuracy:.4f}')\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "296e7abb-86a5-46fd-b034-c4cc11452c0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: chameleon\n",
      "Num. nodes: 2277\n",
      "Num. edges: 18050\n",
      "Num. node features: 2325\n",
      "Num. classes: 5\n",
      "Dataset len.: 1\n",
      "\n",
      "Epoch: 0\n",
      "------\n",
      "Train loss: 1.6068 | Train acc: 0.2289\n",
      "Val loss: 1.6023 | Val acc: 0.2442\n",
      "\n",
      "Epoch: 100\n",
      "------\n",
      "Train loss: 1.1158 | Train acc: 0.5211\n",
      "Val loss: 1.3586 | Val acc: 0.4335\n",
      "\n",
      "Early stopping...\n",
      "\n",
      "Epoch: 0\n",
      "------\n",
      "Train loss: 1.6062 | Train acc: 0.2216\n",
      "Val loss: 1.6091 | Val acc: 0.1756\n",
      "\n",
      "Epoch: 100\n",
      "------\n",
      "Train loss: 1.1046 | Train acc: 0.5559\n",
      "Val loss: 1.4613 | Val acc: 0.4198\n",
      "\n",
      "Early stopping...\n",
      "\n",
      "Epoch: 0\n",
      "------\n",
      "Train loss: 1.6140 | Train acc: 0.2015\n",
      "Val loss: 1.6101 | Val acc: 0.1989\n",
      "\n",
      "Epoch: 100\n",
      "------\n",
      "Train loss: 1.0780 | Train acc: 0.5778\n",
      "Val loss: 1.3291 | Val acc: 0.4472\n",
      "\n",
      "Early stopping...\n",
      "\n",
      "Epoch: 0\n",
      "------\n",
      "Train loss: 1.6127 | Train acc: 0.1648\n",
      "Val loss: 1.6105 | Val acc: 0.1646\n",
      "\n",
      "Epoch: 100\n",
      "------\n",
      "Train loss: 1.1212 | Train acc: 0.5174\n",
      "Val loss: 1.3822 | Val acc: 0.4321\n",
      "\n",
      "Early stopping...\n",
      "\n",
      "Epoch: 0\n",
      "------\n",
      "Train loss: 1.6168 | Train acc: 0.1914\n",
      "Val loss: 1.6148 | Val acc: 0.1920\n",
      "\n",
      "Epoch: 100\n",
      "------\n",
      "Train loss: 1.1276 | Train acc: 0.5284\n",
      "Val loss: 1.3371 | Val acc: 0.4362\n",
      "\n",
      "Early stopping...\n",
      "\n",
      "Epoch: 0\n",
      "------\n",
      "Train loss: 1.6067 | Train acc: 0.2125\n",
      "Val loss: 1.6083 | Val acc: 0.1948\n",
      "\n",
      "Epoch: 100\n",
      "------\n",
      "Train loss: 1.0486 | Train acc: 0.5815\n",
      "Val loss: 1.3454 | Val acc: 0.4335\n",
      "\n",
      "Early stopping...\n",
      "\n",
      "Epoch: 0\n",
      "------\n",
      "Train loss: 1.6138 | Train acc: 0.2015\n",
      "Val loss: 1.6148 | Val acc: 0.1975\n",
      "\n",
      "Epoch: 100\n",
      "------\n",
      "Train loss: 0.9877 | Train acc: 0.6282\n",
      "Val loss: 1.3627 | Val acc: 0.4376\n",
      "\n",
      "Early stopping...\n",
      "\n",
      "Epoch: 0\n",
      "------\n",
      "Train loss: 1.6148 | Train acc: 0.1923\n",
      "Val loss: 1.6120 | Val acc: 0.2016\n",
      "\n",
      "Epoch: 100\n",
      "------\n",
      "Train loss: 1.1442 | Train acc: 0.5375\n",
      "Val loss: 1.3933 | Val acc: 0.4239\n",
      "\n",
      "Early stopping...\n",
      "\n",
      "Epoch: 0\n",
      "------\n",
      "Train loss: 1.6112 | Train acc: 0.1648\n",
      "Val loss: 1.6110 | Val acc: 0.2250\n",
      "\n",
      "Epoch: 100\n",
      "------\n",
      "Train loss: 1.0378 | Train acc: 0.5989\n",
      "Val loss: 1.3027 | Val acc: 0.4499\n",
      "\n",
      "Early stopping...\n",
      "\n",
      "Epoch: 0\n",
      "------\n",
      "Train loss: 1.6119 | Train acc: 0.2198\n",
      "Val loss: 1.6112 | Val acc: 0.2332\n",
      "\n",
      "Epoch: 100\n",
      "------\n",
      "Train loss: 1.1670 | Train acc: 0.5073\n",
      "Val loss: 1.3468 | Val acc: 0.4115\n",
      "\n",
      "Early stopping...\n",
      "Mean Test Accuracy: 0.5252\n",
      "Standard Deviation of Test Accuracies: 0.0478\n"
     ]
    }
   ],
   "source": [
    "# Load the Chameleon dataset for full-supervised task: \n",
    "dataset = get_dataset(path = \"/tmp/chameleon\", name=\"chameleon\",split = \"full\", transform=T.NormalizeFeatures())\n",
    "data = dataset[0]\n",
    "\n",
    "\n",
    "# Training configuration of GCNII for full-supervised task based on Table 7 of the paper\n",
    "SEED = 42\n",
    "NLAYERS = 8\n",
    "ALPHA = 0.2\n",
    "LEARNING_RATE = 0.01\n",
    "NHIDDEN = 64\n",
    "LAMBDA = 1.5\n",
    "DROPOUT = 0.5\n",
    "MAX_EPOCHS = 1500\n",
    "WEIGHT_DECAY = 0.0005\n",
    "EARLY_STOPPING = 100\n",
    "\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "test_accuracies = []\n",
    "\n",
    "# Run the training and evaluation process 10 times with random splits\n",
    "for i in range(10):\n",
    "    # Get a random split of the dataset\n",
    "    Split = i\n",
    "    \n",
    "    \n",
    "    # Initialize the model\n",
    "    model = GCNII(nfeat=data.num_node_features,\n",
    "                  nlayers=NLAYERS,\n",
    "                  nhidden=NHIDDEN,\n",
    "                  nclass=dataset.num_classes,\n",
    "                  dropout=DROPOUT,\n",
    "                  lamda=LAMBDA,\n",
    "                  alpha=ALPHA)\n",
    "    \n",
    "    # Define the optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "    # Train and evaluate the model on the current split\n",
    "    history, test_loss, test_acc  = train(model, data, optimizer, max_epochs=MAX_EPOCHS, early_stopping=EARLY_STOPPING,split = Split)\n",
    "    \n",
    "    # Store the test accuracy result\n",
    "    test_accuracies.append(test_acc)\n",
    "\n",
    "# Calculate the mean and standard deviation of test accuracies\n",
    "mean_accuracy = np.mean(test_accuracies)\n",
    "std_accuracy = np.std(test_accuracies)\n",
    "\n",
    "print(f'Mean Test Accuracy: {mean_accuracy:.4f}')\n",
    "print(f'Standard Deviation of Test Accuracies: {std_accuracy:.4f}')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e146bdc-7a4f-4247-bbc2-9e7036d7b1cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
