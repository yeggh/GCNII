{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b54ac78b-85cb-402c-878f-dbbafbd97116",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, List, Optional, Tuple\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn. functional as F\n",
    "import torch_geometric.transforms as T\n",
    "from torch import Tensor\n",
    "from torch.optim import Optimizer\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.datasets import Planetoid, WebKB\n",
    "from torch_geometric.utils import accuracy\n",
    "from typing_extensions import Literal, TypedDict\n",
    "from collections import defaultdict\n",
    "from typing import List, Optional, Tuple, Union\n",
    "import scipy.sparse\n",
    "from torch import Tensor\n",
    "from scipy.sparse import coo_matrix, eye, diags, csr_matrix\n",
    "import torch.nn as nn\n",
    "from torch.nn import init\n",
    "from torch.nn import Parameter\n",
    "import torch.nn.functional as F\n",
    "from functools import cached_property\n",
    "import scipy.sparse as sp\n",
    "from torch_geometric.transforms import RandomNodeSplit\n",
    "from torch_geometric.datasets.wikipedia_network import WikipediaNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ef577f85-b6dd-4a1d-a337-0c9ad90e0922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load the dataset \n",
    "\n",
    "def get_dataset(path, name, split, transform):\n",
    "    if name in ('Cora', 'CiteSeer', 'PubMed'):\n",
    "        dataset = Planetoid(path, name=name, split=split, transform=transform)\n",
    "    elif name in ('Cornell', 'Texas', 'Wisconsin'):\n",
    "        dataset = WebKB(path, name=name, transform=transform)\n",
    "    elif name == 'chameleon':\n",
    "        dataset = WikipediaNetwork(path, name=name, geom_gcn_preprocess= True, transform=transform)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown dataset name: {name}\")\n",
    "\n",
    "    num_nodes = dataset.data.num_nodes\n",
    "    num_edges = dataset.data.num_edges // 2\n",
    "    print(f\"Dataset: {dataset.name}\")\n",
    "    print(f\"Num. nodes: {num_nodes}\")\n",
    "    print(f\"Num. edges: {num_edges}\")\n",
    "    print(f\"Num. node features: {dataset.num_node_features}\")\n",
    "    print(f\"Num. classes: {dataset.num_classes}\")\n",
    "    print(f\"Dataset len.: {dataset.len()}\")\n",
    "    # print(f\"Sum of row values with normalization: {dataset[0].x.sum(dim=-1)}\")\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6d76a21-f46b-4ad7-8c07-19aafca3ad7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to randomly split nodes of each class into 60%, 20%, and 20% for training, validation and testing based on the paper\n",
    "def split_dataset(dataset, train_ratio=0.6, val_ratio=0.2, test_ratio=0.2):\n",
    "    num_nodes = dataset[0].num_nodes\n",
    "    split = RandomNodeSplit(split=\"random\", num_splits=1, num_train_per_class=int((num_nodes) / 7 * train_ratio), num_val=int(num_nodes * val_ratio), num_test=int(num_nodes * test_ratio))\n",
    "    splitted_dataset = split(dataset[0])\n",
    "    train_len = splitted_dataset.train_mask.sum()\n",
    "    val_len = splitted_dataset.val_mask.sum()\n",
    "    test_len = splitted_dataset.test_mask.sum()\n",
    "    other_len = num_nodes - train_len-val_len - test_len\n",
    "    print(f\"Num. train={train_len}, val={val_len}, test={test_len}, other={other_len}\")\n",
    "    return splitted_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aed475f8-9db7-4a7f-a08c-c3ba40e05b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create an adjacency matrix from edge index and edge attributes\n",
    "\n",
    "def adjacency_matrix(edge_index, edge_attr=None, num_nodes=None):\n",
    "    # Calculate the number of nodes if not provided\n",
    "    if num_nodes is None:\n",
    "        num_nodes = int(edge_index.max()) + 1\n",
    "    # If edge_attr does not exist, assign weight=1 to all edges\n",
    "    if edge_attr is None:\n",
    "        edge_attr = torch.ones(edge_index.shape[1], dtype=torch.float)\n",
    "    # Define the sparse adjacency matrix\n",
    "    adj_matrix_sparse = torch.sparse_coo_tensor(edge_index, edge_attr, (num_nodes, num_nodes))\n",
    "    # Convert to a dense matrix\n",
    "    adj_matrix = adj_matrix_sparse.to_dense()\n",
    "    return (adj_matrix + adj_matrix.T) / 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72c79567-fb40-4655-bcb3-8ec14f4c4c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the normalize adjancy matrix based on the paper P ̃ = (D ̃^ −1/2)A ̃(D ̃^ −1/2 )\n",
    "\n",
    "def normalize_adjacency_matrix(A):\n",
    "    # Ensure A is a sparse matrix\n",
    "    if not isinstance(A, csr_matrix):\n",
    "        A = csr_matrix(A)\n",
    "    A = A + sp.eye(A.shape[0])\n",
    "    degrees = np.array(A.sum(axis=1)).flatten()\n",
    "    degrees[degrees == 0] = 1  # Replace 0s with 1s\n",
    "    D_inv_sqrt = diags(1.0 / np.sqrt(degrees))\n",
    "    normalized_A = D_inv_sqrt @ A @ D_inv_sqrt\n",
    "    return normalized_A\n",
    "\n",
    "\n",
    "# Convert the matrix to Tensor\n",
    "def sparse_matrix_to_torch_sparse_tensor(sparse_matrix):\n",
    "    sparse_matrix = sparse_matrix.tocoo()\n",
    "    indices = torch.LongTensor(np.vstack((sparse_matrix.row, sparse_matrix.col)))\n",
    "    values = torch.FloatTensor(sparse_matrix.data)\n",
    "    shape = torch.Size(sparse_matrix.shape)\n",
    "    return torch.sparse.FloatTensor(indices, values, shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e97b9162-fdc8-4fd5-bdb8-bd04a77da246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition the CGNII model\n",
    "\n",
    "class GCNLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, in_features, out_features):\n",
    "\n",
    "        super(GCNLayer, self).__init__() \n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = torch.nn.Parameter(init.kaiming_uniform_(torch.empty(self.in_features, self.out_features), mode='fan_in', nonlinearity='relu'))\n",
    "\n",
    "    def forward(self, input, adj , h_0 , lamda, alpha, l):\n",
    "\n",
    "        h_l = torch.spmm(adj, input)\n",
    "        features = (1 - alpha) * h_l + alpha * h_0\n",
    "        n = self.weight.shape[0]\n",
    "        I_n = torch.eye(n) \n",
    "        beta = np.log((lamda / l) + 1)\n",
    "        term1 = (1 - beta) * I_n\n",
    "        term2 = beta * self.weight\n",
    "        weights = term1 + term2\n",
    "        output = torch.mm(features, weights)\n",
    "        return output\n",
    "\n",
    "class GCNII(nn.Module):\n",
    "    def __init__(self, nfeat, nlayers, nhidden, nclass, dropout, lamda, alpha):\n",
    "        super(GCNII, self).__init__()\n",
    "        self.graph_convs = nn.ModuleList()  \n",
    "        for i in range(nlayers):\n",
    "            conv_layer = GCNLayer(nhidden, nhidden)\n",
    "            self.graph_convs.append(conv_layer)\n",
    "\n",
    "        self.pre_fc = nn.Linear(nfeat, nhidden)\n",
    "        self.post_fc = nn.Linear(nhidden, nclass)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = dropout\n",
    "        self.lamda = lamda\n",
    "        self.alpha = alpha\n",
    "\n",
    "    # Forward pass accepts edge_index and edge_attr\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        # Construct the adjacency matrix from edge_index and edge_attr\n",
    "        adj = adjacency_matrix(edge_index, edge_attr)\n",
    "        adj = normalize_adjacency_matrix(adj)\n",
    "        adj = sparse_matrix_to_torch_sparse_tensor(adj)\n",
    "\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        h_0 = self.relu(self.pre_fc(x))\n",
    "        h = h_0\n",
    "        for i, con in enumerate(self.graph_convs):\n",
    "            h = F.dropout(h, self.dropout, training=self.training)\n",
    "            h = self.relu(con(h, adj, h_0, self.lamda, self.alpha, i + 1))\n",
    "        h = F.dropout(h, self.dropout, training=self.training)\n",
    "        h = self.post_fc(h)\n",
    "        return F.log_softmax(h, dim=1)\n",
    "    \n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    pass\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d4ef943-7334-410f-8112-0ddfad3611d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "LossFunction = Callable[[Tensor, Tensor], Tensor]\n",
    "Stage = Literal[\"train\", \"val\", \"test\"]\n",
    "\n",
    "def train_step(\n",
    "    model: torch.nn.Module, data: Data, optimizer: torch.optim.Optimizer, loss_function: LossFunction\n",
    ") -> Tuple[float, float]:\n",
    "    # Set model to training mode\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    training_mask = data.train_mask\n",
    "    logits = model(data.x, data.edge_index, data.edge_attr)[training_mask]\n",
    "    predictions = torch.argmax(logits, dim=1)\n",
    "    labels = data.y[training_mask]\n",
    "    loss = loss_function(logits, labels)\n",
    "    acc = accuracy(predictions, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()  \n",
    "    return loss.item(), acc\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_step(\n",
    "    model: torch.nn.Module, data: Data, loss_function: LossFunction, stage: Stage\n",
    ") -> Tuple[float, float]:\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "    stage_mask = getattr(data, f\"{stage}_mask\")\n",
    "    logits = model(data.x, data.edge_index, data.edge_attr)[stage_mask]\n",
    "    predictions = torch.argmax(logits, dim=1)  \n",
    "    labels = data.y[stage_mask]\n",
    "    loss = loss_function(logits, labels)\n",
    "    acc = accuracy(predictions, labels)  \n",
    "    return loss.item(), acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "56a8b538-13bf-424e-b04f-453720932cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HistoryDict(TypedDict):\n",
    "    loss: List[float]\n",
    "    acc: List[float]\n",
    "    val_loss: List[float]\n",
    "    val_acc: List[float]\n",
    "\n",
    "def train(\n",
    "    model: torch.nn.Module,\n",
    "    data: Data,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    loss_function: LossFunction = torch.nn.CrossEntropyLoss(),\n",
    "    max_epochs: int = 1500,\n",
    "    early_stopping: int = 100,\n",
    "    print_interval: int = 100,\n",
    "    verbose: bool = True,\n",
    ") -> HistoryDict:\n",
    "    history = {\"loss\": [], \"val_loss\": [], \"acc\": [], \"val_acc\": []}\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(max_epochs):\n",
    "        # Perform a training step\n",
    "        loss, acc = train_step(model, data, optimizer, loss_function)\n",
    "        # Perform an evaluation step\n",
    "        val_loss, val_acc = evaluate_step(model, data, loss_function, \"val\")\n",
    "\n",
    "        # Update history\n",
    "        history[\"loss\"].append(loss)\n",
    "        history[\"acc\"].append(acc)\n",
    "        history[\"val_loss\"].append(val_loss)\n",
    "        history[\"val_acc\"].append(val_acc)\n",
    "\n",
    "        if epoch > early_stopping and val_loss > np.mean(history[\"val_loss\"][-(early_stopping + 1):-1]):\n",
    "            if verbose:\n",
    "                print(\"\\nEarly stopping...\")\n",
    "            break\n",
    "\n",
    "        # Print training progress\n",
    "        if verbose and epoch % print_interval == 0:\n",
    "            print(f\"\\nEpoch: {epoch}\\n------\")\n",
    "            print(f\"Train loss: {loss:.4f} | Train acc: {acc:.4f}\")\n",
    "            print(f\"Val loss: {val_loss:.4f} | Val acc: {val_acc:.4f}\")\n",
    "\n",
    "    # Perform a final evaluation step\n",
    "    test_loss, test_acc = evaluate_step(model, data, loss_function, \"test\")\n",
    "    # Print final results\n",
    "    # if verbose:\n",
    "    #     print(f\"\\nEpoch: {epoch}\\n------\")\n",
    "    #     print(f\" Train loss: {loss:.4f} | Train acc: {acc:.4f}\")\n",
    "    #     print(f\" Val loss: {val_loss:.4f} | Val acc: {val_acc:.4f}\")\n",
    "    #     print(f\" Test loss: {test_loss:.4f} | Test acc: {test_acc:.4f}\")\n",
    "    return history, test_loss, test_acc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d65c9ca9-fa23-4bd9-aff9-3a8ba04ac98d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: Cora\n",
      "Num. nodes: 2708\n",
      "Num. edges: 5278\n",
      "Num. node features: 1433\n",
      "Num. classes: 7\n",
      "Dataset len.: 1\n",
      "Num. train=1557, val=541, test=541, other=69\n",
      "\n",
      "Epoch: 0\n",
      "------\n",
      "Train loss: 1.9467 | Train acc: 0.1821\n",
      "Val loss: 1.9314 | Val acc: 0.3160\n",
      "\n",
      "Epoch: 100\n",
      "------\n",
      "Train loss: 0.5565 | Train acc: 0.8369\n",
      "Val loss: 0.5107 | Val acc: 0.8680\n",
      "\n",
      "Epoch: 200\n",
      "------\n",
      "Train loss: 0.4231 | Train acc: 0.8783\n",
      "Val loss: 0.4190 | Val acc: 0.8820\n",
      "\n",
      "Epoch: 300\n",
      "------\n",
      "Train loss: 0.4396 | Train acc: 0.8932\n",
      "Val loss: 0.3984 | Val acc: 0.8780\n",
      "\n",
      "Early stopping...\n",
      "Num. train=1557, val=541, test=541, other=69\n",
      "\n",
      "Epoch: 0\n",
      "------\n",
      "Train loss: 1.9386 | Train acc: 0.1680\n",
      "Val loss: 1.9222 | Val acc: 0.3180\n",
      "\n",
      "Epoch: 100\n",
      "------\n",
      "Train loss: 0.5540 | Train acc: 0.8601\n",
      "Val loss: 0.4915 | Val acc: 0.8740\n",
      "\n",
      "Epoch: 200\n",
      "------\n",
      "Train loss: 0.4149 | Train acc: 0.8816\n",
      "Val loss: 0.4075 | Val acc: 0.8800\n",
      "\n",
      "Epoch: 300\n",
      "------\n",
      "Train loss: 0.3927 | Train acc: 0.8791\n",
      "Val loss: 0.3875 | Val acc: 0.8780\n",
      "\n",
      "Early stopping...\n",
      "Num. train=1557, val=541, test=541, other=69\n",
      "\n",
      "Epoch: 0\n",
      "------\n",
      "Train loss: 1.9476 | Train acc: 0.0762\n",
      "Val loss: 1.9334 | Val acc: 0.1620\n",
      "\n",
      "Epoch: 100\n",
      "------\n",
      "Train loss: 0.5705 | Train acc: 0.8493\n",
      "Val loss: 0.4989 | Val acc: 0.8740\n",
      "\n",
      "Epoch: 200\n",
      "------\n",
      "Train loss: 0.4325 | Train acc: 0.8882\n",
      "Val loss: 0.4070 | Val acc: 0.8760\n",
      "\n",
      "Early stopping...\n",
      "Num. train=1557, val=541, test=541, other=69\n",
      "\n",
      "Epoch: 0\n",
      "------\n",
      "Train loss: 1.9527 | Train acc: 0.1531\n",
      "Val loss: 1.9414 | Val acc: 0.1560\n",
      "\n",
      "Epoch: 100\n",
      "------\n",
      "Train loss: 0.5924 | Train acc: 0.8526\n",
      "Val loss: 0.4949 | Val acc: 0.8760\n",
      "\n",
      "Epoch: 200\n",
      "------\n",
      "Train loss: 0.4161 | Train acc: 0.8858\n",
      "Val loss: 0.4132 | Val acc: 0.8780\n",
      "\n",
      "Epoch: 300\n",
      "------\n",
      "Train loss: 0.3764 | Train acc: 0.8998\n",
      "Val loss: 0.3888 | Val acc: 0.8800\n",
      "\n",
      "Early stopping...\n",
      "Num. train=1557, val=541, test=541, other=69\n",
      "\n",
      "Epoch: 0\n",
      "------\n",
      "Train loss: 1.9378 | Train acc: 0.1912\n",
      "Val loss: 1.9249 | Val acc: 0.3160\n",
      "\n",
      "Epoch: 100\n",
      "------\n",
      "Train loss: 0.5650 | Train acc: 0.8526\n",
      "Val loss: 0.5061 | Val acc: 0.8760\n",
      "\n",
      "Epoch: 200\n",
      "------\n",
      "Train loss: 0.4390 | Train acc: 0.8866\n",
      "Val loss: 0.4140 | Val acc: 0.8740\n",
      "\n",
      "Epoch: 300\n",
      "------\n",
      "Train loss: 0.3824 | Train acc: 0.8998\n",
      "Val loss: 0.3948 | Val acc: 0.8760\n",
      "\n",
      "Early stopping...\n",
      "Num. train=1557, val=541, test=541, other=69\n",
      "\n",
      "Epoch: 0\n",
      "------\n",
      "Train loss: 1.9622 | Train acc: 0.1614\n",
      "Val loss: 1.9495 | Val acc: 0.1560\n",
      "\n",
      "Epoch: 100\n",
      "------\n",
      "Train loss: 0.6071 | Train acc: 0.8344\n",
      "Val loss: 0.5097 | Val acc: 0.8740\n",
      "\n",
      "Epoch: 200\n",
      "------\n",
      "Train loss: 0.4240 | Train acc: 0.8858\n",
      "Val loss: 0.4166 | Val acc: 0.8680\n",
      "\n",
      "Epoch: 300\n",
      "------\n",
      "Train loss: 0.3696 | Train acc: 0.8982\n",
      "Val loss: 0.3952 | Val acc: 0.8840\n",
      "\n",
      "Early stopping...\n",
      "Num. train=1557, val=541, test=541, other=69\n",
      "\n",
      "Epoch: 0\n",
      "------\n",
      "Train loss: 1.9550 | Train acc: 0.0745\n",
      "Val loss: 1.9401 | Val acc: 0.0780\n",
      "\n",
      "Epoch: 100\n",
      "------\n",
      "Train loss: 0.5636 | Train acc: 0.8560\n",
      "Val loss: 0.5016 | Val acc: 0.8680\n",
      "\n",
      "Epoch: 200\n",
      "------\n",
      "Train loss: 0.4375 | Train acc: 0.8949\n",
      "Val loss: 0.4200 | Val acc: 0.8740\n",
      "\n",
      "Epoch: 300\n",
      "------\n",
      "Train loss: 0.3908 | Train acc: 0.8949\n",
      "Val loss: 0.3920 | Val acc: 0.8760\n",
      "\n",
      "Early stopping...\n",
      "Num. train=1557, val=541, test=541, other=69\n",
      "\n",
      "Epoch: 0\n",
      "------\n",
      "Train loss: 1.9457 | Train acc: 0.1639\n",
      "Val loss: 1.9363 | Val acc: 0.2220\n",
      "\n",
      "Epoch: 100\n",
      "------\n",
      "Train loss: 0.5892 | Train acc: 0.8336\n",
      "Val loss: 0.4997 | Val acc: 0.8680\n",
      "\n",
      "Epoch: 200\n",
      "------\n",
      "Train loss: 0.4283 | Train acc: 0.8816\n",
      "Val loss: 0.4093 | Val acc: 0.8820\n",
      "\n",
      "Epoch: 300\n",
      "------\n",
      "Train loss: 0.3979 | Train acc: 0.8858\n",
      "Val loss: 0.3877 | Val acc: 0.8740\n",
      "\n",
      "Early stopping...\n",
      "Num. train=1557, val=541, test=541, other=69\n",
      "\n",
      "Epoch: 0\n",
      "------\n",
      "Train loss: 1.9292 | Train acc: 0.2757\n",
      "Val loss: 1.9119 | Val acc: 0.3160\n",
      "\n",
      "Epoch: 100\n",
      "------\n",
      "Train loss: 0.5464 | Train acc: 0.8444\n",
      "Val loss: 0.4854 | Val acc: 0.8760\n",
      "\n",
      "Epoch: 200\n",
      "------\n",
      "Train loss: 0.4206 | Train acc: 0.8849\n",
      "Val loss: 0.4068 | Val acc: 0.8780\n",
      "\n",
      "Epoch: 300\n",
      "------\n",
      "Train loss: 0.3722 | Train acc: 0.9139\n",
      "Val loss: 0.3868 | Val acc: 0.8840\n",
      "\n",
      "Early stopping...\n",
      "Num. train=1557, val=541, test=541, other=69\n",
      "\n",
      "Epoch: 0\n",
      "------\n",
      "Train loss: 1.9423 | Train acc: 0.1349\n",
      "Val loss: 1.9305 | Val acc: 0.3160\n",
      "\n",
      "Epoch: 100\n",
      "------\n",
      "Train loss: 0.5586 | Train acc: 0.8502\n",
      "Val loss: 0.4966 | Val acc: 0.8780\n",
      "\n",
      "Epoch: 200\n",
      "------\n",
      "Train loss: 0.4446 | Train acc: 0.8849\n",
      "Val loss: 0.4124 | Val acc: 0.8840\n",
      "\n",
      "Epoch: 300\n",
      "------\n",
      "Train loss: 0.3524 | Train acc: 0.8982\n",
      "Val loss: 0.3945 | Val acc: 0.8780\n",
      "\n",
      "Early stopping...\n",
      "Mean Test Accuracy: 0.8789\n",
      "Standard Deviation of Test Accuracies: 0.0042\n"
     ]
    }
   ],
   "source": [
    "# Load the Cora dataset for full-supervised task: \n",
    "dataset = get_dataset(path = \"/tmp/Cora\", name=\"Cora\",split = \"full\", transform=T.NormalizeFeatures())\n",
    "data = dataset[0]\n",
    "\n",
    "\n",
    "# Training configuration of GCNII for semi-supervised task based on Table 6 of the paper\n",
    "SEED = 42\n",
    "NLAYERS = 64\n",
    "ALPHA = 0.2\n",
    "LEARNING_RATE = 0.01\n",
    "NHIDDEN = 64\n",
    "LAMBDA = 0.5\n",
    "DROPOUT = 0.5\n",
    "MAX_EPOCHS = 1500\n",
    "WEIGHT_DECAY = 0.0001\n",
    "EARLY_STOPPING = 100\n",
    "\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "\n",
    "\n",
    "test_accuracies = []\n",
    "\n",
    "# Run the training and evaluation process 10 times with random splits\n",
    "for i in range(10):\n",
    "    # Get a random split of the dataset\n",
    "    split_data = split_dataset(dataset)\n",
    "    \n",
    "    \n",
    "    # Initialize the model\n",
    "    model = GCNII(nfeat=split_data.num_node_features,\n",
    "                  nlayers=NLAYERS,\n",
    "                  nhidden=NHIDDEN,\n",
    "                  nclass=dataset.num_classes,\n",
    "                  dropout=DROPOUT,\n",
    "                  lamda=LAMBDA,\n",
    "                  alpha=ALPHA)\n",
    "    \n",
    "    # Define the optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "    # Train and evaluate the model on the current split\n",
    "    history, test_loss, test_acc  = train(model, data, optimizer, max_epochs=MAX_EPOCHS, early_stopping=EARLY_STOPPING)\n",
    "    \n",
    "    # Store the test accuracy result\n",
    "    test_accuracies.append(test_acc)\n",
    "\n",
    "# Calculate the mean and standard deviation of test accuracies\n",
    "mean_accuracy = np.mean(test_accuracies)\n",
    "std_accuracy = np.std(test_accuracies)\n",
    "\n",
    "print(f'Mean Test Accuracy: {mean_accuracy:.4f}')\n",
    "print(f'Standard Deviation of Test Accuracies: {std_accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5f0a5d62-5382-4555-b33d-af0f1c53c6bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: CiteSeer\n",
      "Num. nodes: 3327\n",
      "Num. edges: 4552\n",
      "Num. node features: 3703\n",
      "Num. classes: 6\n",
      "Dataset len.: 1\n",
      "Num. train=1689, val=665, test=665, other=308\n",
      "\n",
      "Epoch: 0\n",
      "------\n",
      "Train loss: 1.8080 | Train acc: 0.0865\n",
      "Val loss: 1.8034 | Val acc: 0.0580\n",
      "\n",
      "Epoch: 100\n",
      "------\n",
      "Train loss: 0.3530 | Train acc: 0.9015\n",
      "Val loss: 0.6692 | Val acc: 0.7820\n",
      "\n",
      "Early stopping...\n",
      "Num. train=1689, val=665, test=665, other=308\n",
      "\n",
      "Epoch: 0\n",
      "------\n",
      "Train loss: 1.7809 | Train acc: 0.2102\n",
      "Val loss: 1.7668 | Val acc: 0.2600\n",
      "\n",
      "Epoch: 100\n",
      "------\n",
      "Train loss: 0.3259 | Train acc: 0.9059\n",
      "Val loss: 0.6661 | Val acc: 0.7720\n",
      "\n",
      "Early stopping...\n",
      "Num. train=1689, val=665, test=665, other=308\n",
      "\n",
      "Epoch: 0\n",
      "------\n",
      "Train loss: 1.8004 | Train acc: 0.0865\n",
      "Val loss: 1.7952 | Val acc: 0.0600\n",
      "\n",
      "Epoch: 100\n",
      "------\n",
      "Train loss: 0.3509 | Train acc: 0.8955\n",
      "Val loss: 0.6759 | Val acc: 0.7800\n",
      "\n",
      "Early stopping...\n",
      "Num. train=1689, val=665, test=665, other=308\n",
      "\n",
      "Epoch: 0\n",
      "------\n",
      "Train loss: 1.7846 | Train acc: 0.1762\n",
      "Val loss: 1.7735 | Val acc: 0.2320\n",
      "\n",
      "Epoch: 100\n",
      "------\n",
      "Train loss: 0.3459 | Train acc: 0.8900\n",
      "Val loss: 0.6691 | Val acc: 0.7820\n",
      "\n",
      "Early stopping...\n",
      "Num. train=1689, val=665, test=665, other=308\n",
      "\n",
      "Epoch: 0\n",
      "------\n",
      "Train loss: 1.7934 | Train acc: 0.1554\n",
      "Val loss: 1.7844 | Val acc: 0.2940\n",
      "\n",
      "Epoch: 100\n",
      "------\n",
      "Train loss: 0.3496 | Train acc: 0.8966\n",
      "Val loss: 0.6666 | Val acc: 0.7800\n",
      "\n",
      "Early stopping...\n",
      "Num. train=1689, val=665, test=665, other=308\n",
      "\n",
      "Epoch: 0\n",
      "------\n",
      "Train loss: 1.7927 | Train acc: 0.1992\n",
      "Val loss: 1.7850 | Val acc: 0.2120\n",
      "\n",
      "Epoch: 100\n",
      "------\n",
      "Train loss: 0.3332 | Train acc: 0.9015\n",
      "Val loss: 0.6698 | Val acc: 0.7800\n",
      "\n",
      "Early stopping...\n",
      "Num. train=1689, val=665, test=665, other=308\n",
      "\n",
      "Epoch: 0\n",
      "------\n",
      "Train loss: 1.7967 | Train acc: 0.2031\n",
      "Val loss: 1.7877 | Val acc: 0.2320\n",
      "\n",
      "Epoch: 100\n",
      "------\n",
      "Train loss: 0.3535 | Train acc: 0.8944\n",
      "Val loss: 0.6661 | Val acc: 0.7820\n",
      "\n",
      "Early stopping...\n",
      "Num. train=1689, val=665, test=665, other=308\n",
      "\n",
      "Epoch: 0\n",
      "------\n",
      "Train loss: 1.7908 | Train acc: 0.1549\n",
      "Val loss: 1.7824 | Val acc: 0.2740\n",
      "\n",
      "Epoch: 100\n",
      "------\n",
      "Train loss: 0.3504 | Train acc: 0.8883\n",
      "Val loss: 0.6578 | Val acc: 0.7840\n",
      "\n",
      "Early stopping...\n",
      "Num. train=1689, val=665, test=665, other=308\n",
      "\n",
      "Epoch: 0\n",
      "------\n",
      "Train loss: 1.7998 | Train acc: 0.0865\n",
      "Val loss: 1.7941 | Val acc: 0.1120\n",
      "\n",
      "Epoch: 100\n",
      "------\n",
      "Train loss: 0.3410 | Train acc: 0.8966\n",
      "Val loss: 0.6667 | Val acc: 0.7860\n",
      "\n",
      "Early stopping...\n",
      "Num. train=1689, val=665, test=665, other=308\n",
      "\n",
      "Epoch: 0\n",
      "------\n",
      "Train loss: 1.8030 | Train acc: 0.1768\n",
      "Val loss: 1.7976 | Val acc: 0.2320\n",
      "\n",
      "Epoch: 100\n",
      "------\n",
      "Train loss: 0.3513 | Train acc: 0.8971\n",
      "Val loss: 0.6667 | Val acc: 0.7780\n",
      "\n",
      "Early stopping...\n",
      "Mean Test Accuracy: 0.7899\n",
      "Standard Deviation of Test Accuracies: 0.0031\n"
     ]
    }
   ],
   "source": [
    "# Load the CiteSeer dataset for full-supervised task: \n",
    "\n",
    "dataset = get_dataset(path = \"/tmp/CiteSeer\", name=\"CiteSeer\",split = \"full\", transform=T.NormalizeFeatures())\n",
    "data = dataset[0]\n",
    "\n",
    "\n",
    "# Training configuration of GCNII for semi-supervised task based on Table 6 of the paper\n",
    "SEED = 42\n",
    "NLAYERS = 64\n",
    "ALPHA = 0.5\n",
    "LEARNING_RATE = 0.01\n",
    "NHIDDEN = 64\n",
    "LAMBDA = 0.5\n",
    "DROPOUT = 0.5\n",
    "MAX_EPOCHS = 1500\n",
    "WEIGHT_DECAY = 5e-6\n",
    "EARLY_STOPPING = 100\n",
    "\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "test_accuracies = []\n",
    "\n",
    "# Run the training and evaluation process 10 times with random splits\n",
    "for i in range(10):\n",
    "    # Get a random split of the dataset\n",
    "    split_data = split_dataset(dataset)\n",
    "    \n",
    "    \n",
    "    # Initialize the model\n",
    "    model = GCNII(nfeat=split_data.num_node_features,\n",
    "                  nlayers=NLAYERS,\n",
    "                  nhidden=NHIDDEN,\n",
    "                  nclass=dataset.num_classes,\n",
    "                  dropout=DROPOUT,\n",
    "                  lamda=LAMBDA,\n",
    "                  alpha=ALPHA)\n",
    "    \n",
    "    # Define the optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "    # Train and evaluate the model on the current split\n",
    "    history, test_loss, test_acc  = train(model, data, optimizer, max_epochs=MAX_EPOCHS, early_stopping=EARLY_STOPPING)\n",
    "    \n",
    "    # Store the test accuracy result\n",
    "    test_accuracies.append(test_acc)\n",
    "\n",
    "# Calculate the mean and standard deviation of test accuracies\n",
    "mean_accuracy = np.mean(test_accuracies)\n",
    "std_accuracy = np.std(test_accuracies)\n",
    "\n",
    "print(f'Mean Test Accuracy: {mean_accuracy:.4f}')\n",
    "print(f'Standard Deviation of Test Accuracies: {std_accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "30822c1f-dc46-4165-ac8e-5b9f9fdff965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: PubMed\n",
      "Num. nodes: 19717\n",
      "Num. edges: 44324\n",
      "Num. node features: 500\n",
      "Num. classes: 3\n",
      "Dataset len.: 1\n",
      "Num. train=5070, val=3943, test=3943, other=6761\n",
      "\n",
      "Epoch: 0\n",
      "------\n",
      "Train loss: 1.1077 | Train acc: 0.2150\n",
      "Val loss: 1.0984 | Val acc: 0.3880\n",
      "\n",
      "Epoch: 100\n",
      "------\n",
      "Train loss: 0.4354 | Train acc: 0.8470\n",
      "Val loss: 0.3315 | Val acc: 0.8900\n",
      "\n",
      "Epoch: 200\n",
      "------\n",
      "Train loss: 0.3954 | Train acc: 0.8567\n",
      "Val loss: 0.2995 | Val acc: 0.8980\n",
      "\n",
      "Epoch: 300\n",
      "------\n",
      "Train loss: 0.3786 | Train acc: 0.8631\n",
      "Val loss: 0.2839 | Val acc: 0.8980\n",
      "\n",
      "Epoch: 400\n",
      "------\n",
      "Train loss: 0.3643 | Train acc: 0.8686\n",
      "Val loss: 0.2710 | Val acc: 0.9020\n",
      "\n",
      "Early stopping...\n",
      "Num. train=5070, val=3943, test=3943, other=6761\n",
      "\n",
      "Epoch: 0\n",
      "------\n",
      "Train loss: 1.0969 | Train acc: 0.3977\n",
      "Val loss: 1.0850 | Val acc: 0.4240\n",
      "\n",
      "Epoch: 100\n",
      "------\n",
      "Train loss: 0.4375 | Train acc: 0.8470\n",
      "Val loss: 0.3323 | Val acc: 0.8900\n",
      "\n",
      "Epoch: 200\n",
      "------\n",
      "Train loss: 0.3904 | Train acc: 0.8608\n",
      "Val loss: 0.2967 | Val acc: 0.8980\n",
      "\n",
      "Epoch: 300\n",
      "------\n",
      "Train loss: 0.3825 | Train acc: 0.8634\n",
      "Val loss: 0.2760 | Val acc: 0.9020\n",
      "\n",
      "Early stopping...\n",
      "Num. train=5070, val=3943, test=3943, other=6761\n",
      "\n",
      "Epoch: 0\n",
      "------\n",
      "Train loss: 1.0903 | Train acc: 0.3982\n",
      "Val loss: 1.0794 | Val acc: 0.4160\n",
      "\n",
      "Epoch: 100\n",
      "------\n",
      "Train loss: 0.4323 | Train acc: 0.8487\n",
      "Val loss: 0.3312 | Val acc: 0.8940\n",
      "\n",
      "Epoch: 200\n",
      "------\n",
      "Train loss: 0.3882 | Train acc: 0.8627\n",
      "Val loss: 0.2937 | Val acc: 0.9000\n",
      "\n",
      "Epoch: 300\n",
      "------\n",
      "Train loss: 0.3704 | Train acc: 0.8672\n",
      "Val loss: 0.2775 | Val acc: 0.9040\n",
      "\n",
      "Early stopping...\n",
      "Num. train=5070, val=3943, test=3943, other=6761\n",
      "\n",
      "Epoch: 0\n",
      "------\n",
      "Train loss: 1.1043 | Train acc: 0.3914\n",
      "Val loss: 1.0951 | Val acc: 0.3880\n",
      "\n",
      "Epoch: 100\n",
      "------\n",
      "Train loss: 0.4358 | Train acc: 0.8449\n",
      "Val loss: 0.3307 | Val acc: 0.8940\n",
      "\n",
      "Epoch: 200\n",
      "------\n",
      "Train loss: 0.3903 | Train acc: 0.8620\n",
      "Val loss: 0.2952 | Val acc: 0.9000\n",
      "\n",
      "Epoch: 300\n",
      "------\n",
      "Train loss: 0.3747 | Train acc: 0.8663\n",
      "Val loss: 0.2773 | Val acc: 0.9040\n",
      "\n",
      "Early stopping...\n",
      "Num. train=5070, val=3943, test=3943, other=6761\n",
      "\n",
      "Epoch: 0\n",
      "------\n",
      "Train loss: 1.0934 | Train acc: 0.3979\n",
      "Val loss: 1.0823 | Val acc: 0.4160\n",
      "\n",
      "Epoch: 100\n",
      "------\n",
      "Train loss: 0.4275 | Train acc: 0.8490\n",
      "Val loss: 0.3251 | Val acc: 0.8880\n",
      "\n",
      "Epoch: 200\n",
      "------\n",
      "Train loss: 0.3899 | Train acc: 0.8607\n",
      "Val loss: 0.2943 | Val acc: 0.8980\n",
      "\n",
      "Epoch: 300\n",
      "------\n",
      "Train loss: 0.3815 | Train acc: 0.8641\n",
      "Val loss: 0.2774 | Val acc: 0.9020\n",
      "\n",
      "Early stopping...\n",
      "Num. train=5070, val=3943, test=3943, other=6761\n",
      "\n",
      "Epoch: 0\n",
      "------\n",
      "Train loss: 1.0998 | Train acc: 0.3986\n",
      "Val loss: 1.0863 | Val acc: 0.4160\n",
      "\n",
      "Epoch: 100\n",
      "------\n",
      "Train loss: 0.4357 | Train acc: 0.8459\n",
      "Val loss: 0.3302 | Val acc: 0.8940\n",
      "\n",
      "Epoch: 200\n",
      "------\n",
      "Train loss: 0.3953 | Train acc: 0.8603\n",
      "Val loss: 0.2973 | Val acc: 0.8980\n",
      "\n",
      "Epoch: 300\n",
      "------\n",
      "Train loss: 0.3728 | Train acc: 0.8674\n",
      "Val loss: 0.2784 | Val acc: 0.9060\n",
      "\n",
      "Epoch: 400\n",
      "------\n",
      "Train loss: 0.3661 | Train acc: 0.8706\n",
      "Val loss: 0.2671 | Val acc: 0.9080\n",
      "\n",
      "Early stopping...\n",
      "Num. train=5070, val=3943, test=3943, other=6761\n",
      "\n",
      "Epoch: 0\n",
      "------\n",
      "Train loss: 1.0868 | Train acc: 0.4023\n",
      "Val loss: 1.0782 | Val acc: 0.4240\n",
      "\n",
      "Epoch: 100\n",
      "------\n",
      "Train loss: 0.4326 | Train acc: 0.8473\n",
      "Val loss: 0.3334 | Val acc: 0.8820\n",
      "\n",
      "Epoch: 200\n",
      "------\n",
      "Train loss: 0.3940 | Train acc: 0.8615\n",
      "Val loss: 0.2948 | Val acc: 0.9020\n",
      "\n",
      "Epoch: 300\n",
      "------\n",
      "Train loss: 0.3751 | Train acc: 0.8669\n",
      "Val loss: 0.2780 | Val acc: 0.9060\n",
      "\n",
      "Early stopping...\n",
      "Num. train=5070, val=3943, test=3943, other=6761\n",
      "\n",
      "Epoch: 0\n",
      "------\n",
      "Train loss: 1.0807 | Train acc: 0.3905\n",
      "Val loss: 1.0704 | Val acc: 0.3880\n",
      "\n",
      "Epoch: 100\n",
      "------\n",
      "Train loss: 0.4230 | Train acc: 0.8490\n",
      "Val loss: 0.3269 | Val acc: 0.8940\n",
      "\n",
      "Epoch: 200\n",
      "------\n",
      "Train loss: 0.3891 | Train acc: 0.8620\n",
      "Val loss: 0.2933 | Val acc: 0.8980\n",
      "\n",
      "Epoch: 300\n",
      "------\n",
      "Train loss: 0.3732 | Train acc: 0.8686\n",
      "Val loss: 0.2770 | Val acc: 0.9100\n",
      "\n",
      "Early stopping...\n",
      "Num. train=5070, val=3943, test=3943, other=6761\n",
      "\n",
      "Epoch: 0\n",
      "------\n",
      "Train loss: 1.0937 | Train acc: 0.3915\n",
      "Val loss: 1.0850 | Val acc: 0.3880\n",
      "\n",
      "Epoch: 100\n",
      "------\n",
      "Train loss: 0.4330 | Train acc: 0.8456\n",
      "Val loss: 0.3293 | Val acc: 0.8880\n",
      "\n",
      "Epoch: 200\n",
      "------\n",
      "Train loss: 0.3911 | Train acc: 0.8600\n",
      "Val loss: 0.2936 | Val acc: 0.8960\n",
      "\n",
      "Epoch: 300\n",
      "------\n",
      "Train loss: 0.3731 | Train acc: 0.8686\n",
      "Val loss: 0.2780 | Val acc: 0.9020\n",
      "\n",
      "Epoch: 400\n",
      "------\n",
      "Train loss: 0.3639 | Train acc: 0.8716\n",
      "Val loss: 0.2693 | Val acc: 0.9040\n",
      "\n",
      "Early stopping...\n",
      "Num. train=5070, val=3943, test=3943, other=6761\n",
      "\n",
      "Epoch: 0\n",
      "------\n",
      "Train loss: 1.0977 | Train acc: 0.3904\n",
      "Val loss: 1.0890 | Val acc: 0.3880\n",
      "\n",
      "Epoch: 100\n",
      "------\n",
      "Train loss: 0.4333 | Train acc: 0.8463\n",
      "Val loss: 0.3323 | Val acc: 0.8920\n",
      "\n",
      "Epoch: 200\n",
      "------\n",
      "Train loss: 0.3878 | Train acc: 0.8631\n",
      "Val loss: 0.2947 | Val acc: 0.9020\n",
      "\n",
      "Epoch: 300\n",
      "------\n",
      "Train loss: 0.3746 | Train acc: 0.8670\n",
      "Val loss: 0.2822 | Val acc: 0.8980\n",
      "\n",
      "Early stopping...\n",
      "Mean Test Accuracy: 0.8864\n",
      "Standard Deviation of Test Accuracies: 0.0027\n"
     ]
    }
   ],
   "source": [
    "# Load the PubMed dataset for full-supervised task: \n",
    "dataset = get_dataset(path = \"/tmp/PubMed\", name=\"PubMed\",split = \"full\", transform=T.NormalizeFeatures())\n",
    "data = dataset[0]\n",
    "\n",
    "\n",
    "# Training configuration of GCNII for semi-supervised task based on Table 6 of the paper\n",
    "SEED = 42\n",
    "NLAYERS = 64\n",
    "ALPHA = 0.1\n",
    "LEARNING_RATE = 0.01\n",
    "NHIDDEN = 64\n",
    "LAMBDA = 0.5\n",
    "DROPOUT = 0.5\n",
    "MAX_EPOCHS = 1500\n",
    "WEIGHT_DECAY = 5e-6\n",
    "EARLY_STOPPING = 100\n",
    "\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "test_accuracies = []\n",
    "\n",
    "# Run the training and evaluation process 10 times with random splits\n",
    "for i in range(10):\n",
    "    # Get a random split of the dataset\n",
    "    split_data = split_dataset(dataset)\n",
    "    \n",
    "    \n",
    "    # Initialize the model\n",
    "    model = GCNII(nfeat=split_data.num_node_features,\n",
    "                  nlayers=NLAYERS,\n",
    "                  nhidden=NHIDDEN,\n",
    "                  nclass=dataset.num_classes,\n",
    "                  dropout=DROPOUT,\n",
    "                  lamda=LAMBDA,\n",
    "                  alpha=ALPHA)\n",
    "    \n",
    "    # Define the optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "    # Train and evaluate the model on the current split\n",
    "    history, test_loss, test_acc  = train(model, data, optimizer, max_epochs=MAX_EPOCHS, early_stopping=EARLY_STOPPING)\n",
    "    \n",
    "    # Store the test accuracy result\n",
    "    test_accuracies.append(test_acc)\n",
    "\n",
    "# Calculate the mean and standard deviation of test accuracies\n",
    "mean_accuracy = np.mean(test_accuracies)\n",
    "std_accuracy = np.std(test_accuracies)\n",
    "\n",
    "print(f'Mean Test Accuracy: {mean_accuracy:.4f}')\n",
    "print(f'Standard Deviation of Test Accuracies: {std_accuracy:.4f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5ad859-764a-4424-8e03-20961c037055",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
