{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b54ac78b-85cb-402c-878f-dbbafbd97116",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, List, Optional, Tuple\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn. functional as F\n",
    "import torch_geometric.transforms as T\n",
    "from torch import Tensor\n",
    "from torch.optim import Optimizer\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.utils import accuracy\n",
    "from typing_extensions import Literal, TypedDict\n",
    "from collections import defaultdict\n",
    "from typing import List, Optional, Tuple, Union\n",
    "import scipy.sparse\n",
    "from torch import Tensor\n",
    "from scipy.sparse import coo_matrix, eye, diags, csr_matrix\n",
    "import torch.nn as nn\n",
    "from torch.nn import init\n",
    "from torch.nn import Parameter\n",
    "import torch.nn.functional as F\n",
    "from functools import cached_property\n",
    "import scipy.sparse as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ee55d3b-3699-421d-bb69-f3d3e4f8b14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load the dataset \n",
    "def get_dataset(path, name,split,num_train_per_class,num_val,num_test, transform):\n",
    "    dataset = Planetoid(path, name=name,split= split,num_train_per_class= num_train_per_class, num_val = num_val, num_test = num_test, transform=transform)\n",
    "    num_nodes = dataset.data.num_nodes\n",
    "    num_edges = dataset.data.num_edges // 2\n",
    "    train_len = dataset[0].train_mask.sum()\n",
    "    val_len = dataset[0].val_mask.sum()\n",
    "    test_len = dataset[0].test_mask.sum()\n",
    "    other_len = num_nodes - train_len-val_len - test_len\n",
    "    print(f\"Dataset: {dataset.name}\")\n",
    "    print(f\"Num. nodes: {num_nodes} (train={train_len}, val={val_len}, test={test_len}, other={other_len})\")\n",
    "    print(f\"Num. edges: {num_edges}\")\n",
    "    print(f\"Num. node features: {dataset.num_node_features}\")\n",
    "    print(f\"Num. classes: {dataset.num_classes}\")\n",
    "    print(f\"Dataset len.: {dataset.len()}\")\n",
    "    # print(f\"Sum of row values with normalization: {dataset[0].x.sum(dim=-1)}\")\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aed475f8-9db7-4a7f-a08c-c3ba40e05b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create an adjacency matrix from edge index and edge attributes\n",
    "\n",
    "def adjacency_matrix(edge_index, edge_attr=None, num_nodes=None):\n",
    "    # Calculate the number of nodes if not provided\n",
    "    if num_nodes is None:\n",
    "        num_nodes = int(edge_index.max()) + 1\n",
    "    # If edge_attr does not exist, assign weight=1 to all edges\n",
    "    if edge_attr is None:\n",
    "        edge_attr = torch.ones(edge_index.shape[1], dtype=torch.float)\n",
    "    # Define the sparse adjacency matrix\n",
    "    adj_matrix_sparse = torch.sparse_coo_tensor(edge_index, edge_attr, (num_nodes, num_nodes))\n",
    "    # Convert to a dense matrix\n",
    "    adj_matrix = adj_matrix_sparse.to_dense()\n",
    "    return (adj_matrix + adj_matrix.T) / 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72c79567-fb40-4655-bcb3-8ec14f4c4c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the normalize adjancy matrix based on the paper P ̃ = (D ̃^ −1/2)A ̃(D ̃^ −1/2 )\n",
    "\n",
    "def normalize_adjacency_matrix(A):\n",
    "    # Ensure A is a sparse matrix\n",
    "    if not isinstance(A, csr_matrix):\n",
    "        A = csr_matrix(A)\n",
    "    A = A + sp.eye(A.shape[0])\n",
    "    degrees = np.array(A.sum(axis=1)).flatten()\n",
    "    degrees[degrees == 0] = 1  # Replace 0s with 1s\n",
    "    D_inv_sqrt = diags(1.0 / np.sqrt(degrees))\n",
    "    normalized_A = D_inv_sqrt @ A @ D_inv_sqrt\n",
    "    return normalized_A\n",
    "\n",
    "\n",
    "# Convert the matrix to Tensor\n",
    "def sparse_matrix_to_torch_sparse_tensor(sparse_matrix):\n",
    "    sparse_matrix = sparse_matrix.tocoo()\n",
    "    indices = torch.LongTensor(np.vstack((sparse_matrix.row, sparse_matrix.col)))\n",
    "    values = torch.FloatTensor(sparse_matrix.data)\n",
    "    shape = torch.Size(sparse_matrix.shape)\n",
    "    return torch.sparse.FloatTensor(indices, values, shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e97b9162-fdc8-4fd5-bdb8-bd04a77da246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition the CGNII model\n",
    "\n",
    "class GCNLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, in_features, out_features):\n",
    "\n",
    "        super(GCNLayer, self).__init__() \n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = torch.nn.Parameter(init.kaiming_uniform_(torch.empty(self.in_features, self.out_features), mode='fan_in', nonlinearity='relu'))\n",
    "\n",
    "    def forward(self, input, adj , h_0 , lamda, alpha, l):\n",
    "\n",
    "        h_l = torch.spmm(adj, input)\n",
    "        features = (1 - alpha) * h_l + alpha * h_0\n",
    "        n = self.weight.shape[0]\n",
    "        I_n = torch.eye(n) \n",
    "        beta = np.log((lamda / l) + 1)\n",
    "        term1 = (1 - beta) * I_n\n",
    "        term2 = beta * self.weight\n",
    "        weights = term1 + term2\n",
    "        output = torch.mm(features, weights)\n",
    "        return output\n",
    "\n",
    "class GCNII(nn.Module):\n",
    "    def __init__(self, nfeat, nlayers, nhidden, nclass, dropout, lamda, alpha):\n",
    "        super(GCNII, self).__init__()\n",
    "        self.graph_convs = nn.ModuleList()  \n",
    "        for i in range(nlayers):\n",
    "            conv_layer = GCNLayer(nhidden, nhidden)\n",
    "            self.graph_convs.append(conv_layer)\n",
    "\n",
    "        self.pre_fc = nn.Linear(nfeat, nhidden)\n",
    "        self.post_fc = nn.Linear(nhidden, nclass)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = dropout\n",
    "        self.lamda = lamda\n",
    "        self.alpha = alpha\n",
    "\n",
    "    # Forward pass accepts edge_index and edge_attr\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        # Construct the adjacency matrix from edge_index and edge_attr\n",
    "        adj = adjacency_matrix(edge_index, edge_attr)\n",
    "        adj = normalize_adjacency_matrix(adj)\n",
    "        adj = sparse_matrix_to_torch_sparse_tensor(adj)\n",
    "\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        h_0 = self.relu(self.pre_fc(x))\n",
    "        h = h_0\n",
    "        for i, con in enumerate(self.graph_convs):\n",
    "            h = F.dropout(h, self.dropout, training=self.training)\n",
    "            h = self.relu(con(h, adj, h_0, self.lamda, self.alpha, i + 1))\n",
    "        h = F.dropout(h, self.dropout, training=self.training)\n",
    "        h = self.post_fc(h)\n",
    "        return F.log_softmax(h, dim=1)\n",
    "    \n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    pass\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d4ef943-7334-410f-8112-0ddfad3611d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "LossFunction = Callable[[Tensor, Tensor], Tensor]\n",
    "Stage = Literal[\"train\", \"val\", \"test\"]\n",
    "\n",
    "def train_step(\n",
    "    model: torch.nn.Module, data: Data, optimizer: torch.optim.Optimizer, loss_function: LossFunction\n",
    ") -> Tuple[float, float]:\n",
    "    # Set model to training mode\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    training_mask = data.train_mask\n",
    "    logits = model(data.x, data.edge_index, data.edge_attr)[training_mask]\n",
    "    predictions = torch.argmax(logits, dim=1)\n",
    "    labels = data.y[training_mask]\n",
    "    loss = loss_function(logits, labels)\n",
    "    acc = accuracy(predictions, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()  \n",
    "    return loss.item(), acc\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_step(\n",
    "    model: torch.nn.Module, data: Data, loss_function: LossFunction, stage: Stage\n",
    ") -> Tuple[float, float]:\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "    stage_mask = getattr(data, f\"{stage}_mask\")\n",
    "    logits = model(data.x, data.edge_index, data.edge_attr)[stage_mask]\n",
    "    predictions = torch.argmax(logits, dim=1)  \n",
    "    labels = data.y[stage_mask]\n",
    "    loss = loss_function(logits, labels)\n",
    "    acc = accuracy(predictions, labels)  \n",
    "    return loss.item(), acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56a8b538-13bf-424e-b04f-453720932cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HistoryDict(TypedDict):\n",
    "    loss: List[float]\n",
    "    acc: List[float]\n",
    "    val_loss: List[float]\n",
    "    val_acc: List[float]\n",
    "\n",
    "def train(\n",
    "    model: torch.nn.Module,\n",
    "    data: Data,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    loss_function: LossFunction = torch.nn.CrossEntropyLoss(),\n",
    "    max_epochs: int = 1500,\n",
    "    early_stopping: int = 100,\n",
    "    print_interval: int = 20,\n",
    "    verbose: bool = True,\n",
    ") -> HistoryDict:\n",
    "    history = {\"loss\": [], \"val_loss\": [], \"acc\": [], \"val_acc\": []}\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(max_epochs):\n",
    "        # Perform a training step\n",
    "        loss, acc = train_step(model, data, optimizer, loss_function)\n",
    "        # Perform an evaluation step\n",
    "        val_loss, val_acc = evaluate_step(model, data, loss_function, \"val\")\n",
    "\n",
    "        # Update history\n",
    "        history[\"loss\"].append(loss)\n",
    "        history[\"acc\"].append(acc)\n",
    "        history[\"val_loss\"].append(val_loss)\n",
    "        history[\"val_acc\"].append(val_acc)\n",
    "\n",
    "        if epoch > early_stopping and val_loss > np.mean(history[\"val_loss\"][-(early_stopping + 1):-1]):\n",
    "            if verbose:\n",
    "                print(\"\\nEarly stopping...\")\n",
    "            break\n",
    "\n",
    "        # Print training progress\n",
    "        if verbose and epoch % print_interval == 0:\n",
    "            print(f\"\\nEpoch: {epoch}\\n------\")\n",
    "            print(f\"Train loss: {loss:.4f} | Train acc: {acc:.4f}\")\n",
    "            print(f\"Val loss: {val_loss:.4f} | Val acc: {val_acc:.4f}\")\n",
    "\n",
    "    # Perform a final evaluation step\n",
    "    test_loss, test_acc = evaluate_step(model, data, loss_function, \"test\")\n",
    "    # Print final results\n",
    "    if verbose:\n",
    "        print(f\"\\nEpoch: {epoch}\\n------\")\n",
    "        print(f\" Train loss: {loss:.4f} | Train acc: {acc:.4f}\")\n",
    "        print(f\" Val loss: {val_loss:.4f} | Val acc: {val_acc:.4f}\")\n",
    "        print(f\" Test loss: {test_loss:.4f} | Test acc: {test_acc:.4f}\")\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d65c9ca9-fa23-4bd9-aff9-3a8ba04ac98d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: Cora\n",
      "Num. nodes: 2708 (train=140, val=500, test=1000, other=1068)\n",
      "Num. edges: 5278\n",
      "Num. node features: 1433\n",
      "Num. classes: 7\n",
      "Dataset len.: 1\n",
      "\n",
      "Epoch: 0\n",
      "------\n",
      "Train loss: 1.9482 | Train acc: 0.1429\n",
      "Val loss: 1.9327 | Val acc: 0.1620\n",
      "\n",
      "Epoch: 20\n",
      "------\n",
      "Train loss: 1.8855 | Train acc: 0.4143\n",
      "Val loss: 1.8920 | Val acc: 0.7440\n",
      "\n",
      "Epoch: 40\n",
      "------\n",
      "Train loss: 1.7371 | Train acc: 0.4429\n",
      "Val loss: 1.7675 | Val acc: 0.7280\n",
      "\n",
      "Epoch: 60\n",
      "------\n",
      "Train loss: 1.6112 | Train acc: 0.5214\n",
      "Val loss: 1.5907 | Val acc: 0.7840\n",
      "\n",
      "Epoch: 80\n",
      "------\n",
      "Train loss: 1.3724 | Train acc: 0.6571\n",
      "Val loss: 1.4288 | Val acc: 0.7940\n",
      "\n",
      "Epoch: 100\n",
      "------\n",
      "Train loss: 1.2164 | Train acc: 0.6500\n",
      "Val loss: 1.3062 | Val acc: 0.8000\n",
      "\n",
      "Epoch: 120\n",
      "------\n",
      "Train loss: 1.1287 | Train acc: 0.7071\n",
      "Val loss: 1.2148 | Val acc: 0.8120\n",
      "\n",
      "Epoch: 140\n",
      "------\n",
      "Train loss: 1.0902 | Train acc: 0.7143\n",
      "Val loss: 1.1625 | Val acc: 0.8040\n",
      "\n",
      "Epoch: 160\n",
      "------\n",
      "Train loss: 1.1445 | Train acc: 0.6786\n",
      "Val loss: 1.1021 | Val acc: 0.8140\n",
      "\n",
      "Epoch: 180\n",
      "------\n",
      "Train loss: 0.9534 | Train acc: 0.7571\n",
      "Val loss: 1.0675 | Val acc: 0.8140\n",
      "\n",
      "Epoch: 200\n",
      "------\n",
      "Train loss: 1.0411 | Train acc: 0.6786\n",
      "Val loss: 1.0553 | Val acc: 0.8200\n",
      "\n",
      "Epoch: 220\n",
      "------\n",
      "Train loss: 0.9425 | Train acc: 0.7571\n",
      "Val loss: 1.0131 | Val acc: 0.8100\n",
      "\n",
      "Epoch: 240\n",
      "------\n",
      "Train loss: 0.9155 | Train acc: 0.7714\n",
      "Val loss: 0.9980 | Val acc: 0.8160\n",
      "\n",
      "Epoch: 260\n",
      "------\n",
      "Train loss: 0.9242 | Train acc: 0.7571\n",
      "Val loss: 0.9786 | Val acc: 0.8200\n",
      "\n",
      "Epoch: 280\n",
      "------\n",
      "Train loss: 0.8602 | Train acc: 0.7714\n",
      "Val loss: 0.9664 | Val acc: 0.8120\n",
      "\n",
      "Epoch: 300\n",
      "------\n",
      "Train loss: 0.9110 | Train acc: 0.7571\n",
      "Val loss: 0.9535 | Val acc: 0.8160\n",
      "\n",
      "Epoch: 320\n",
      "------\n",
      "Train loss: 0.9560 | Train acc: 0.7000\n",
      "Val loss: 0.9518 | Val acc: 0.8160\n",
      "\n",
      "Epoch: 340\n",
      "------\n",
      "Train loss: 0.9057 | Train acc: 0.7571\n",
      "Val loss: 0.9362 | Val acc: 0.8120\n",
      "\n",
      "Epoch: 360\n",
      "------\n",
      "Train loss: 0.8619 | Train acc: 0.7429\n",
      "Val loss: 0.9271 | Val acc: 0.8160\n",
      "\n",
      "Epoch: 380\n",
      "------\n",
      "Train loss: 0.8512 | Train acc: 0.7786\n",
      "Val loss: 0.8942 | Val acc: 0.8160\n",
      "\n",
      "Epoch: 400\n",
      "------\n",
      "Train loss: 0.8211 | Train acc: 0.7571\n",
      "Val loss: 0.9183 | Val acc: 0.8160\n",
      "\n",
      "Early stopping...\n",
      "\n",
      "Epoch: 401\n",
      "------\n",
      " Train loss: 0.9676 | Train acc: 0.7714\n",
      " Val loss: 0.9252 | Val acc: 0.8200\n",
      " Test loss: 0.8966 | Test acc: 0.8380\n"
     ]
    }
   ],
   "source": [
    "# Load the Cora dataset semi-supervised task: \n",
    "# 20 nodes per class for training, 500 nodes for validation and 1000 nodes for testing based on the paper\n",
    "dataset = get_dataset(path = \"/tmp/Cora\", name=\"Cora\",split = \"public\",num_train_per_class= 20, num_val = 500, num_test = 1000, transform=T.NormalizeFeatures())\n",
    "data = dataset[0]\n",
    "\n",
    "\n",
    "# Training configuration of GCNII for semi-supervised task based on Table 6 of the paper\n",
    "SEED = 42\n",
    "NLAYERS = 64\n",
    "ALPHA = 0.1\n",
    "LEARNING_RATE = 0.01\n",
    "NHIDDEN = 64\n",
    "LAMBDA = 0.5\n",
    "DROPOUT = 0.6\n",
    "MAX_EPOCHS = 1500\n",
    "WEIGHT_DECAY_1 = 0.01\n",
    "WEIGHT_DECAY_2 = 5e-4\n",
    "EARLY_STOPPING = 100\n",
    "\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# Initialize the model\n",
    "model = GCNII(nfeat=data.num_node_features,\n",
    "              nlayers=NLAYERS,\n",
    "              nhidden=NHIDDEN,\n",
    "              nclass=dataset.num_classes,\n",
    "              dropout=DROPOUT,\n",
    "              lamda=LAMBDA,\n",
    "              alpha=ALPHA)\n",
    "\n",
    "# Define the optimizer\n",
    "opt_order_dict = []\n",
    "for name, param in model.named_parameters():\n",
    "    if name.startswith('graph'):\n",
    "        opt_order_dict.append({'params':param,'weight_decay':WEIGHT_DECAY_1})\n",
    "    else:\n",
    "        opt_order_dict.append({'params':param,'weight_decay':WEIGHT_DECAY_2})\n",
    "\n",
    "optimizer = torch.optim.Adam(opt_order_dict,lr=LEARNING_RATE)\n",
    "\n",
    "# Train the model\n",
    "history = train(model, data, optimizer, max_epochs=MAX_EPOCHS, early_stopping=EARLY_STOPPING)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f0a5d62-5382-4555-b33d-af0f1c53c6bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: CiteSeer\n",
      "Num. nodes: 3327 (train=120, val=500, test=1000, other=1707)\n",
      "Num. edges: 4552\n",
      "Num. node features: 3703\n",
      "Num. classes: 6\n",
      "Dataset len.: 1\n",
      "\n",
      "Epoch: 0\n",
      "------\n",
      "Train loss: 1.7907 | Train acc: 0.1667\n",
      "Val loss: 1.8000 | Val acc: 0.1380\n",
      "\n",
      "Epoch: 20\n",
      "------\n",
      "Train loss: 1.7766 | Train acc: 0.2667\n",
      "Val loss: 1.7700 | Val acc: 0.4940\n",
      "\n",
      "Epoch: 40\n",
      "------\n",
      "Train loss: 1.7258 | Train acc: 0.3833\n",
      "Val loss: 1.7273 | Val acc: 0.6320\n",
      "\n",
      "Epoch: 60\n",
      "------\n",
      "Train loss: 1.6503 | Train acc: 0.5167\n",
      "Val loss: 1.6773 | Val acc: 0.6560\n",
      "\n",
      "Epoch: 80\n",
      "------\n",
      "Train loss: 1.6013 | Train acc: 0.5250\n",
      "Val loss: 1.6129 | Val acc: 0.6980\n",
      "\n",
      "Epoch: 100\n",
      "------\n",
      "Train loss: 1.4856 | Train acc: 0.5417\n",
      "Val loss: 1.5400 | Val acc: 0.7200\n",
      "\n",
      "Epoch: 120\n",
      "------\n",
      "Train loss: 1.4208 | Train acc: 0.5667\n",
      "Val loss: 1.4817 | Val acc: 0.7040\n",
      "\n",
      "Epoch: 140\n",
      "------\n",
      "Train loss: 1.3819 | Train acc: 0.5500\n",
      "Val loss: 1.4352 | Val acc: 0.7260\n",
      "\n",
      "Epoch: 160\n",
      "------\n",
      "Train loss: 1.5765 | Train acc: 0.5667\n",
      "Val loss: 1.4039 | Val acc: 0.7240\n",
      "\n",
      "Epoch: 180\n",
      "------\n",
      "Train loss: 1.3193 | Train acc: 0.6333\n",
      "Val loss: 1.3910 | Val acc: 0.7240\n",
      "\n",
      "Epoch: 200\n",
      "------\n",
      "Train loss: 1.3788 | Train acc: 0.5083\n",
      "Val loss: 1.3579 | Val acc: 0.7000\n",
      "\n",
      "Epoch: 220\n",
      "------\n",
      "Train loss: 1.3282 | Train acc: 0.5917\n",
      "Val loss: 1.3665 | Val acc: 0.6880\n",
      "\n",
      "Epoch: 240\n",
      "------\n",
      "Train loss: 1.4549 | Train acc: 0.6167\n",
      "Val loss: 1.3175 | Val acc: 0.7180\n",
      "\n",
      "Epoch: 260\n",
      "------\n",
      "Train loss: 1.2172 | Train acc: 0.6500\n",
      "Val loss: 1.2927 | Val acc: 0.7180\n",
      "\n",
      "Epoch: 280\n",
      "------\n",
      "Train loss: 1.2417 | Train acc: 0.6667\n",
      "Val loss: 1.3129 | Val acc: 0.7000\n",
      "\n",
      "Epoch: 300\n",
      "------\n",
      "Train loss: 1.7276 | Train acc: 0.5917\n",
      "Val loss: 1.2963 | Val acc: 0.7140\n",
      "\n",
      "Epoch: 320\n",
      "------\n",
      "Train loss: 1.2425 | Train acc: 0.6500\n",
      "Val loss: 1.2960 | Val acc: 0.7140\n",
      "\n",
      "Early stopping...\n",
      "\n",
      "Epoch: 324\n",
      "------\n",
      " Train loss: 1.8851 | Train acc: 0.6583\n",
      " Val loss: 1.3029 | Val acc: 0.7060\n",
      " Test loss: 1.2806 | Test acc: 0.7050\n"
     ]
    }
   ],
   "source": [
    "# Load the CiteSeer dataset semi-supervised task: \n",
    "# 20 nodes per class for training, 500 nodes for validation and 1000 nodes for testing based on the paper\n",
    "dataset = get_dataset(path = \"/tmp/CiteSeer\", name=\"CiteSeer\",split = \"public\",num_train_per_class= 20, num_val = 500, num_test = 1000, transform=T.NormalizeFeatures())\n",
    "data = dataset[0]\n",
    "\n",
    "\n",
    "# Training configuration of GCNII for semi-supervised task based on Table 6 of the paper\n",
    "SEED = 42\n",
    "NLAYERS = 32\n",
    "ALPHA = 0.1\n",
    "LEARNING_RATE = 0.01\n",
    "NHIDDEN = 256\n",
    "LAMBDA = 0.6\n",
    "DROPOUT = 0.7\n",
    "MAX_EPOCHS = 1500\n",
    "WEIGHT_DECAY_1 = 0.01\n",
    "WEIGHT_DECAY_2 = 5e-4\n",
    "EARLY_STOPPING = 100\n",
    "\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# Initialize the model\n",
    "model = GCNII(nfeat=data.num_node_features,\n",
    "              nlayers=NLAYERS,\n",
    "              nhidden=NHIDDEN,\n",
    "              nclass=dataset.num_classes,\n",
    "              dropout=DROPOUT,\n",
    "              lamda=LAMBDA,\n",
    "              alpha=ALPHA)\n",
    "\n",
    "# Define the optimizer\n",
    "opt_order_dict = []\n",
    "for name, param in model.named_parameters():\n",
    "    if name.startswith('graph'):\n",
    "        opt_order_dict.append({'params':param,'weight_decay':WEIGHT_DECAY_1})\n",
    "    else:\n",
    "        opt_order_dict.append({'params':param,'weight_decay':WEIGHT_DECAY_2})\n",
    "\n",
    "optimizer = torch.optim.Adam(opt_order_dict,lr=LEARNING_RATE)\n",
    "\n",
    "\n",
    "# Train the model\n",
    "history = train(model, data, optimizer, max_epochs=MAX_EPOCHS, early_stopping=EARLY_STOPPING)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "30822c1f-dc46-4165-ac8e-5b9f9fdff965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: PubMed\n",
      "Num. nodes: 19717 (train=60, val=500, test=1000, other=18157)\n",
      "Num. edges: 44324\n",
      "Num. node features: 500\n",
      "Num. classes: 3\n",
      "Dataset len.: 1\n",
      "\n",
      "Epoch: 0\n",
      "------\n",
      "Train loss: 1.1015 | Train acc: 0.3167\n",
      "Val loss: 1.1014 | Val acc: 0.1960\n",
      "\n",
      "Epoch: 20\n",
      "------\n",
      "Train loss: 0.6526 | Train acc: 0.7833\n",
      "Val loss: 0.7568 | Val acc: 0.7380\n",
      "\n",
      "Epoch: 40\n",
      "------\n",
      "Train loss: 0.2104 | Train acc: 0.9667\n",
      "Val loss: 0.6174 | Val acc: 0.7600\n",
      "\n",
      "Epoch: 60\n",
      "------\n",
      "Train loss: 0.2138 | Train acc: 0.9667\n",
      "Val loss: 0.5089 | Val acc: 0.8220\n",
      "\n",
      "Epoch: 80\n",
      "------\n",
      "Train loss: 0.4884 | Train acc: 0.8500\n",
      "Val loss: 1.2037 | Val acc: 0.6540\n",
      "\n",
      "Epoch: 100\n",
      "------\n",
      "Train loss: 0.1750 | Train acc: 0.9500\n",
      "Val loss: 0.6008 | Val acc: 0.7960\n",
      "\n",
      "Epoch: 120\n",
      "------\n",
      "Train loss: 0.0979 | Train acc: 0.9833\n",
      "Val loss: 0.5167 | Val acc: 0.8120\n",
      "\n",
      "Early stopping...\n",
      "\n",
      "Epoch: 126\n",
      "------\n",
      " Train loss: 0.1215 | Train acc: 0.9833\n",
      " Val loss: 0.6233 | Val acc: 0.7840\n",
      " Test loss: 0.6624 | Test acc: 0.7800\n"
     ]
    }
   ],
   "source": [
    "# Load the PubMed dataset semi-supervised task: \n",
    "# 20 nodes per class for training, 500 nodes for validation and 1000 nodes for testing based on the paper\n",
    "dataset = get_dataset(path = \"/tmp/PubMed\", name=\"PubMed\",split = \"public\",num_train_per_class= 20, num_val = 500, num_test = 1000, transform=T.NormalizeFeatures())\n",
    "data = dataset[0]\n",
    "\n",
    "\n",
    "# Training configuration of GCNII for semi-supervised task based on Table 6 of the paper\n",
    "SEED = 42\n",
    "NLAYERS = 16\n",
    "ALPHA = 0.1\n",
    "LEARNING_RATE = 0.01\n",
    "NHIDDEN = 256\n",
    "LAMBDA = 0.4\n",
    "DROPOUT = 0.5\n",
    "MAX_EPOCHS = 1500\n",
    "WEIGHT_DECAY_1 = 5e-4\n",
    "WEIGHT_DECAY_2 = 5e-4\n",
    "EARLY_STOPPING = 100\n",
    "\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# Initialize the model\n",
    "model = GCNII(nfeat=data.num_node_features,\n",
    "              nlayers=NLAYERS,\n",
    "              nhidden=NHIDDEN,\n",
    "              nclass=dataset.num_classes,\n",
    "              dropout=DROPOUT,\n",
    "              lamda=LAMBDA,\n",
    "              alpha=ALPHA)\n",
    "\n",
    "# Define the optimizer\n",
    "opt_order_dict = []\n",
    "for name, param in model.named_parameters():\n",
    "    if name.startswith('graph'):\n",
    "        opt_order_dict.append({'params':param,'weight_decay':WEIGHT_DECAY_1})\n",
    "    else:\n",
    "        opt_order_dict.append({'params':param,'weight_decay':WEIGHT_DECAY_2})\n",
    "\n",
    "optimizer = torch.optim.Adam(opt_order_dict,lr=LEARNING_RATE)\n",
    "\n",
    "# Train the model\n",
    "history = train(model, data, optimizer, max_epochs=MAX_EPOCHS, early_stopping=EARLY_STOPPING)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5ad859-764a-4424-8e03-20961c037055",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
